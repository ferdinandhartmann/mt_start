{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCarEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Define Action Space (Discrete Example) ---\n",
    "        # 0: Forward, 1: Left, 2: Right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.action_velocities = {\n",
    "            0: (10.0, 10.0), # Forward\n",
    "            1: (10.0, 10.0), # Left\n",
    "            2: (10.0, 1.0)  # Right\n",
    "        }\n",
    "\n",
    "        # --- Define Observation Space ---\n",
    "        # Example: [car_x, car_y, car_yaw, target_x, target_y]\n",
    "        # Needs careful normalization/scaling! Box space is common.\n",
    "        # Define reasonable low/high bounds based on your expected maze size\n",
    "        low = np.array([-10, -10, -np.pi, -10, -10], dtype=np.float32)\n",
    "        high = np.array([10, 10, np.pi, 10, 10], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # --- PyBullet Setup ---\n",
    "        self.render_mode = render_mode\n",
    "        self.client = p.connect(p.DIRECT if render_mode is None else p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
    "        # Define goal areas (use class attributes)\n",
    "        self.goal_area_1 = np.array([5, 5]) # Store only XY\n",
    "        self.goal_area_2 = np.array([5, -5])\n",
    "        self.goal_radius = 0.5\n",
    "        self.target_goal_pos = None # Will be set in reset()\n",
    "        self.correct_goal_index = -1 # 0 for goal 1, 1 for goal 2\n",
    "        # Visualize goals (only works in GUI mode)\n",
    "        if p.getConnectionInfo()['connectionMethod'] == p.GUI:\n",
    "            goal_visual_shape_1 = p.createVisualShape(p.GEOM_SPHERE, radius=self.goal_radius, rgbaColor=[0, 1, 0, 0.5])\n",
    "            goal_visual_shape_2 = p.createVisualShape(p.GEOM_SPHERE, radius=self.goal_radius, rgbaColor=[1, 0, 0, 0.5])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_1, basePosition=[5, 5, 0.1])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_2, basePosition=[5, -5, 0.1])\n",
    "\n",
    "        # Load car (ensure path is correct)\n",
    "        self.start_pos = [0,0,0.1]\n",
    "        self.start_orn = p.getQuaternionFromEuler([0,0,0])\n",
    "        self.carId = p.loadURDF(\"/urdf/simple_two_wheel_car.urdf\", self.start_pos, self.start_orn, physicsClientId=self.client)\n",
    "        # FIND YOUR JOINT INDICES HERE (as done in Phase 1)\n",
    "        self.left_wheel_joint_index = 1 # Replace with actual index\n",
    "        self.right_wheel_joint_index = 0 # Replace with actual index\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.max_steps_per_episode = 5000 # Limit episode length\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        euler = p.getEulerFromQuaternion(orn_quat)\n",
    "        yaw = euler[2]\n",
    "        # Return observation matching the defined space\n",
    "        return np.array([pos[0], pos[1], yaw, self.target_goal_pos[0], self.target_goal_pos[1]], dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        # Optional: provide extra info not used for learning\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        dist_goal1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1)\n",
    "        dist_goal2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2)\n",
    "        return {\"distance_goal1\": dist_goal1, \"distance_goal2\": dist_goal2, \"target_goal_index\": self.correct_goal_index}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Reset car position/orientation\n",
    "        p.resetBasePositionAndOrientation(self.carId, self.start_pos, self.start_orn, physicsClientId=self.client)\n",
    "        p.resetBaseVelocity(self.carId, linearVelocity=[0,0,0], angularVelocity=[0,0,0], physicsClientId=self.client)\n",
    "        # Reset wheel velocities (important!)\n",
    "        self._set_wheel_velocities(0, 0) # Stop wheels\n",
    "\n",
    "        # --- Randomly choose the CORRECT goal for this episode ---\n",
    "        self.correct_goal_index = self.np_random.integers(0, 2) # 0 or 1\n",
    "        if self.correct_goal_index == 0:\n",
    "            self.target_goal_pos = self.goal_area_1\n",
    "        else:\n",
    "            self.target_goal_pos = self.goal_area_2\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Optional: Add goal visualization if rendering\n",
    "        if self.render_mode == \"human\":\n",
    "             self._render_frame() # Might need specific visualization logic here\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def _set_wheel_velocities(self, left_vel, right_vel):\n",
    "         p.setJointMotorControl2(bodyUniqueId=self.carId,\n",
    "                                jointIndex=self.left_wheel_joint_index,\n",
    "                                controlMode=p.VELOCITY_CONTROL,\n",
    "                                targetVelocity=left_vel,\n",
    "                                physicsClientId=self.client)\n",
    "         p.setJointMotorControl2(bodyUniqueId=self.carId,\n",
    "                                jointIndex=self.right_wheel_joint_index,\n",
    "                                controlMode=p.VELOCITY_CONTROL,\n",
    "                                targetVelocity=right_vel,\n",
    "                                physicsClientId=self.client)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Apply action (set wheel velocities)\n",
    "        left_vel, right_vel = self.action_velocities[action]\n",
    "        self._set_wheel_velocities(left_vel, right_vel)\n",
    "\n",
    "        # Step simulation\n",
    "        if action in [1, 2]:  # Left or Right turn\n",
    "            for _ in range(100):  # Adjust the range to fine-tune the turn angle\n",
    "                p.stepSimulation(physicsClientId=self.client)\n",
    "\n",
    "        # Step simulation for forward movement\n",
    "        else:\n",
    "            p.stepSimulation(physicsClientId=self.client)\n",
    "\n",
    "        self.step_counter += 1\n",
    "\n",
    "        # Get observation, calculate reward, check termination/truncation\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Check goal conditions\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        in_goal_1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1) < self.goal_radius\n",
    "        in_goal_2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2) < self.goal_radius\n",
    "\n",
    "        terminated = False\n",
    "        reward = -0.1 # Small penalty per step\n",
    "\n",
    "        if in_goal_1:\n",
    "            if self.correct_goal_index == 0:\n",
    "                reward = 100.0 # Reached correct goal 1\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -50.0 # Reached incorrect goal 1\n",
    "                terminated = True # End episode even if wrong goal is reached\n",
    "        elif in_goal_2:\n",
    "            if self.correct_goal_index == 1:\n",
    "                reward = 100.0 # Reached correct goal 2\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -50.0 # Reached incorrect goal 2\n",
    "                terminated = True\n",
    "\n",
    "        # Check truncation (max steps exceeded)\n",
    "        truncated = self.step_counter >= self.max_steps_per_episode\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        # terminated: Episode ended successfully (goal) or unsuccessfully (wrong goal, crash - add later)\n",
    "        # truncated: Episode ended due to external limit (time limit)\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # PyBullet handles rendering if connected with p.GUI\n",
    "        # This method might be needed for specific Gym requirements or offscreen rendering\n",
    "        if self.render_mode == \"human\":\n",
    "             # Usually PyBullet's GUI handles this, but you might add overlays here\n",
    "             pass\n",
    "        # Implement other modes like 'rgb_array' if needed\n",
    "        return None # Or an image array\n",
    "\n",
    "    def _render_frame(self):\n",
    "         # Could add debug text, lines etc. using p.addUserDebugText, p.addUserDebugLine\n",
    "         pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(physicsClientId=self.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check passed!\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO # Or A2C, DQN for discrete actions\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "# Instantiate and check the environment\n",
    "env = MazeCarEnv(render_mode=None) # Enable rendering during training\n",
    "# It will check your defined spaces, reset/step methods for compatibility\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "    env.close()\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Define the model (PPO is a good starting point for Box observations)\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./ppo_mazecar_tensorboard/\",\n",
    "            batch_size=64,\n",
    "            learning_rate=0.0003,\n",
    "            n_steps=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: MazeCarEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      7\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;124m'\u001b[39m, eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,\n\u001b[1;32m      9\u001b[0m                              deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m, callback\u001b[38;5;241m=\u001b[39meval_callback)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_mazecar_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: MazeCarEnv(render_mode=\"Human\")])\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=10000,\n",
    "                             deterministic=True, render=False)\n",
    "model.learn(total_timesteps=200000, callback=eval_callback)\n",
    "model.save(\"ppo_mazecar_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "# --- To evaluate ---\n",
    "eval_env.close()\n",
    "eval_env = MazeCarEnv(render_mode=\"human\") # Render during evaluation\n",
    "model = PPO.load(\"ppo_mazecar_model\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "for _ in range(10000): # Max steps for evaluation\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "     # Convert action to the correct format if necessary\n",
    "    if isinstance(action, np.ndarray) and action.size == 1:  # Discrete action\n",
    "        action = action.item()  # Convert to scalar (e.g., 2 instead of [2])\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    time.sleep(1./480.)\n",
    "    if terminated or truncated:\n",
    "        print(f\"Evaluation episode finished. Reached target goal: {info.get('target_goal_index')}, Reward: {reward}\")\n",
    "        obs, info = eval_env.reset() # Reset for next evaluation episode\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresetSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m p\u001b[38;5;241m.\u001b[39mdisconnect()\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XIO:  fatal IO error 62 (Timer expired) on X server \":0\"\n",
      "      after 148211 requests (148211 known processed) with 0 events remaining.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "p.resetSimulation()\n",
    "p.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pybullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
