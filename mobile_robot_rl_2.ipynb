{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO # Or A2C, DQN for discrete actions\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCarEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], \"render_fps\": 500}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Define Action Space ---\n",
    "        # Hier exemplarisch: 2 diskrete Aktionen (links/rechts drehen + vorr√ºcken)\n",
    "        self.action_space = spaces.Discrete(3)  # 0: links, 1: rechts\n",
    "        \n",
    "        # --- Define Observation Space ---\n",
    "        # Beobachtung: [Auto_x, Auto_y, Auto_yaw, Ziel_x, Ziel_y]\n",
    "        low = np.array([-6, -6, -np.pi, -6, -6], dtype=np.float32)\n",
    "        high = np.array([6, 6, np.pi, 6, 6], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # --- PyBullet Setup ---\n",
    "        self.render_mode = render_mode\n",
    "        self.client = p.connect(p.DIRECT if render_mode is None else p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81, physicsClientId=self.client)\n",
    "\n",
    "        # Lade eine Plane als Boden\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
    "\n",
    "        # Maze als URDF laden (dieses sollte z. B. als \"maze.urdf\" im Arbeitsverzeichnis liegen)\n",
    "        # Die Option useFixedBase=True sorgt daf√ºr, dass das Maze statisch bleibt.\n",
    "        self.mazeId = p.loadURDF(\"urdf/maze.urdf\",\n",
    "                                 basePosition=[0, 0, 0],\n",
    "                                 useFixedBase=True,\n",
    "                                 physicsClientId=self.client)\n",
    "\n",
    "        # Ziele definieren (2 verschiedene, beispielhaft)\n",
    "        # Passe sie an die tats√§chlichen G√§nge/Ausg√§nge in deinem Maze an.\n",
    "        self.goal_area_1 = np.array([4.5, 4.5])   # oberer Ausgang\n",
    "        self.goal_area_2 = np.array([4.5, -4.5])  # unterer Ausgang\n",
    "        self.goal_radius = 0.5\n",
    "        self.target_goal_pos = None  # Wird in reset() gesetzt\n",
    "        self.correct_goal_index = self.np_random.integers(0, 2  )  # Zuf√§llige Wahl des Ziels\n",
    "\n",
    "        # Falls im GUI-Modus: Visualisiere die Ziele als farbige Kugeln\n",
    "        if p.getConnectionInfo()['connectionMethod'] == p.GUI:\n",
    "            goal_visual_shape_1 = p.createVisualShape(\n",
    "                p.GEOM_SPHERE,\n",
    "                radius=self.goal_radius,\n",
    "                rgbaColor=[0, 1, 0, 0.5]  # halbdurchsichtig gr√ºn\n",
    "            )\n",
    "            goal_visual_shape_2 = p.createVisualShape(\n",
    "                p.GEOM_SPHERE,\n",
    "                radius=self.goal_radius,\n",
    "                rgbaColor=[1, 0, 0, 0.5]  # halbdurchsichtig rot\n",
    "            )\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_1,\n",
    "                              basePosition=[self.goal_area_1[0], self.goal_area_1[1], 0.1])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_2,\n",
    "                              basePosition=[self.goal_area_2[0], self.goal_area_2[1], 0.1])\n",
    "\n",
    "        # Roboter (Auto) laden\n",
    "        # Passe ggf. den Pfad an, wenn das URDF woanders liegt\n",
    "        self.start_pos = [-4.5, 0.0, 0.1]  # Start nahe der linken Seite des Maze\n",
    "        self.start_orn = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.carId = p.loadURDF(\"urdf/simple_two_wheel_car.urdf\",\n",
    "                                self.start_pos, self.start_orn,\n",
    "                                physicsClientId=self.client)\n",
    "        \n",
    "        # IDs der R√§der (m√ºssen zu deinem Roboter-URDF passen)\n",
    "        self.left_wheel_joint_index = 1\n",
    "        self.right_wheel_joint_index = 0\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.max_steps_per_episode = 1000  # z. B. mehr Schritte erlauben\n",
    "\n",
    "        self.action_repeat = 50\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        euler = p.getEulerFromQuaternion(orn_quat)\n",
    "        yaw = euler[2]\n",
    "        return np.array([pos[0], pos[1], yaw,\n",
    "                         self.target_goal_pos[0],\n",
    "                         self.target_goal_pos[1]], dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        dist_goal1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1)\n",
    "        dist_goal2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2)\n",
    "        return {\n",
    "            \"distance_goal1\": dist_goal1,\n",
    "            \"distance_goal2\": dist_goal2,\n",
    "            \"target_goal_index\": self.correct_goal_index\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Auto zur√ºcksetzen\n",
    "        # Zuf√§llige Startposition leicht variieren (z.‚ÄØB. ¬±0.3 in X/Y)\n",
    "        start_x = self.start_pos[0] + self.np_random.uniform(-0.3, 0.3)\n",
    "        start_y = self.start_pos[1] + self.np_random.uniform(-0.3, 0.3)\n",
    "        p.resetBasePositionAndOrientation(self.carId, [start_x, start_y, self.start_pos[2]], self.start_orn, physicsClientId=self.client)\n",
    "        p.resetBaseVelocity(self.carId,\n",
    "                            linearVelocity=[0, 0, 0],\n",
    "                            angularVelocity=[0, 0, 0],\n",
    "                            physicsClientId=self.client)\n",
    "\n",
    "        # Zuf√§llige Wahl des \"richtigen\" Ziels (oder fest vorgegeben)\n",
    "        # self.correct_goal_index = self.np_random.integers(0, 2)\n",
    "        self.correct_goal_index = 0  # zum Testen immer Goal 1\n",
    "        if self.correct_goal_index == 0:\n",
    "            self.target_goal_pos = self.goal_area_1\n",
    "        else:\n",
    "            self.target_goal_pos = self.goal_area_2\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "            # time.sleep(0.02)\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        self.prev_dist_to_goal = np.linalg.norm(np.array(car_pos[:2]) - self.target_goal_pos)\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        if action == 0:\n",
    "            left_vel, right_vel = 10.0, 10.0\n",
    "        elif action == 1:\n",
    "            left_vel, right_vel = 2.0, 10.0\n",
    "        else:  # action == 2\n",
    "            left_vel, right_vel = 10.0, 2.0\n",
    "\n",
    "        # Setze die Motoren der beiden R√§der\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.left_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=left_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.right_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=right_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "\n",
    "\n",
    "        # print(f\"Action: {action}, Velocities: L={left_vel}, R={right_vel}, Step={self.step_counter}\")\n",
    "        # car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        # print(f\"Position: {car_pos}\")\n",
    "\n",
    "\n",
    "        # Jetzt die Simulation mehrmals updaten,\n",
    "        # damit der Roboter tats√§chlich f√§hrt und gegen W√§nde kollidieren kann\n",
    "        for _ in range(self.action_repeat):\n",
    "            p.stepSimulation()\n",
    "            # if self.render_mode == \"human\":\n",
    "            #     time.sleep(0.001) \n",
    "            #     print(\"Step Simulation\")\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "        # Beobachtung + Reward + Done bestimmen\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # # Check goal conditions\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        in_goal_1 = np.linalg.norm(car_pos[:2] - self.goal_area_1) < self.goal_radius\n",
    "        in_goal_2 = np.linalg.norm(car_pos[:2] - self.goal_area_2) < self.goal_radius\n",
    "\n",
    "        terminated = False\n",
    "        # Neue Distanz zum Ziel\n",
    "        # curr_dist_to_goal = np.linalg.norm(np.array(car_pos[:2]) - self.target_goal_pos)\n",
    "        # reward = self.prev_dist_to_goal - curr_dist_to_goal  # Belohnung f√ºr Ann√§herung\n",
    "\n",
    "        # Kleine Zeitstrafe, damit das Auto schneller f√§hrt\n",
    "        reward = 0.0\n",
    "        reward -= 0.01\n",
    "\n",
    "        # Belohnung aktualisieren\n",
    "        # self.prev_dist_to_goal = curr_dist_to_goal\n",
    "\n",
    "\n",
    "        if in_goal_1:\n",
    "            if self.correct_goal_index == 0:\n",
    "                reward = 50.0\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -10.0\n",
    "                terminated = True\n",
    "        elif in_goal_2:\n",
    "            if self.correct_goal_index == 1:\n",
    "                reward = 50.0\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -10.0\n",
    "                terminated = True\n",
    "\n",
    "        # Nach dem Schritt: Pr√ºfe auf Kollision\n",
    "        # Pr√ºfe auf Kontakt mit dem Maze\n",
    "        contacts = p.getContactPoints(bodyA=self.carId, bodyB=self.mazeId, physicsClientId=self.client)\n",
    "        if len(contacts) > 0:\n",
    "            reward -= 1.0  # Strafe f√ºr Kollision mit Maze-Wand\n",
    "            terminated = True \n",
    "\n",
    "\n",
    "        truncated = (self.step_counter >= self.max_steps_per_episode)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # Bei PyBullet im GUI-Modus passiert das Rendering automatisch\n",
    "        pass\n",
    "\n",
    "    def _render_frame(self):\n",
    "        # Debug-Anzeigen, falls erw√ºnscht\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(physicsClientId=self.client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check passed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "env = MazeCarEnv(render_mode=None) \n",
    "# env = MazeCarEnv(render_mode=\"human\")\n",
    "\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "    env.close()\n",
    "    exit()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env, \n",
    "            # verbose=1, \n",
    "            tensorboard_log=\"./ppo_mazecar_tensorboard/\",\n",
    "            batch_size=256, # after n_steps the data is split into batches\n",
    "            learning_rate=0.0003,\n",
    "            n_steps=1024,   # number of steps collected before a training\n",
    "            device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.callbacks import BaseCallback\n",
    "# import time\n",
    "\n",
    "# class EvalCallback(BaseCallback):\n",
    "#     def __init__(self, eval_env, eval_freq=1000, n_eval_episodes=1, verbose=1):\n",
    "#         super().__init__(verbose)\n",
    "#         self.eval_env = eval_env\n",
    "#         self.eval_freq = eval_freq\n",
    "#         self.n_eval_episodes = n_eval_episodes\n",
    "#         self.best_mean_reward = -float(\"inf\")\n",
    "    \n",
    "#     def _manual_eval(self):\n",
    "#         episode_rewards = []\n",
    "#         for _ in range(self.n_eval_episodes):\n",
    "#             obs, info = self.eval_env.reset()\n",
    "#             done = False\n",
    "#             truncated = False\n",
    "#             total_reward = 0.0\n",
    "#             while not (done or truncated):\n",
    "#                 action, _states = self.model.predict(obs, deterministic=True)\n",
    "#                 obs, reward, done, truncated, info = self.eval_env.step(action)\n",
    "#                 total_reward += reward\n",
    "#                 time.sleep(1. / 120.0)  # Slows simulation so GUI can show movement\n",
    "#             episode_rewards.append(total_reward)\n",
    "#         mean = sum(episode_rewards) / len(episode_rewards)\n",
    "#         std = (sum((r - mean) ** 2 for r in episode_rewards) / len(episode_rewards)) ** 0.5\n",
    "#         return mean, std\n",
    "\n",
    "#     def _on_step(self):\n",
    "#         if self.n_calls % self.eval_freq == 0:\n",
    "#             print(f\"\\nüöÄ Evaluating at step {self.n_calls}\")\n",
    "#             mean_reward, std_reward = self._manual_eval()\n",
    "#             print(f\"‚úÖ Step {self.n_calls}: Mean reward = {mean_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "\n",
    "#             if mean_reward > self.best_mean_reward:\n",
    "#                 self.best_mean_reward = mean_reward\n",
    "#                 self.model.save(f\"best_model_step_{self.n_calls}\")\n",
    "#         return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d9241a417243539559e0d8a8b90c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval_env = MazeCarEnv(render_mode='human')\n",
    "# eval_freq = 1000 \n",
    "# eval_callback = EvalCallback(eval_env, eval_freq, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=500000, progress_bar=True)\n",
    "\n",
    "model.save(\"ppo_mazecar_model_2\")\n",
    "env.close()\n",
    "# check_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -1.01\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -1.01\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "# --- To evaluate ---\n",
    "eval_env = MazeCarEnv(render_mode=\"human\") # Render during evaluation\n",
    "model = PPO.load(\"ppo_mazecar_model_2\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "for _ in range(500): # Max steps for evaluation\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    time.sleep(1./120.0)  # Adjust sleep time for rendering speed\n",
    "    if terminated or truncated:\n",
    "        print(f\"Evaluation episode finished. Reached target goal: {info.get('target_goal_index')}, Reward: {reward}\")\n",
    "        obs, info = eval_env.reset() # Reset for next evaluation episode\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m check_env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m      5\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[0;32mIn[2], line 230\u001b[0m, in \u001b[0;36mMazeCarEnv.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 230\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mphysicsClientId\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "\n",
    "check_env.close()\n",
    "\n",
    "eval_env.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# car_pos, _ = p.getBasePositionAndOrientation(eval_env.carId, physicsClientId=eval_env.client)\n",
    "# p.resetDebugVisualizerCamera(cameraDistance=3, cameraYaw=50, cameraPitch=-35, cameraTargetPosition=car_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/masterthesis/mt_start$   tensorboard --logdir=./ppo_mazecar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.get_device_name(0))  # Prints the name of the GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pybullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
