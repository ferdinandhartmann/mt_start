{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import PPO # Or A2C, DQN for discrete actions\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCarEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Define Action Space ---\n",
    "        # Hier exemplarisch: 2 diskrete Aktionen (links/rechts drehen + vorrücken)\n",
    "        self.action_space = spaces.Discrete(2)  # 0: links, 1: rechts\n",
    "        \n",
    "        # --- Define Observation Space ---\n",
    "        # Beobachtung: [Auto_x, Auto_y, Auto_yaw, Ziel_x, Ziel_y]\n",
    "        low = np.array([-10, -10, -np.pi, -10, -10], dtype=np.float32)\n",
    "        high = np.array([10, 10, np.pi, 10, 10], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # --- PyBullet Setup ---\n",
    "        self.render_mode = render_mode\n",
    "        self.client = p.connect(p.DIRECT if render_mode is None else p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81, physicsClientId=self.client)\n",
    "\n",
    "        # Lade eine Plane als Boden\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
    "\n",
    "        # Maze als URDF laden (dieses sollte z. B. als \"maze.urdf\" im Arbeitsverzeichnis liegen)\n",
    "        # Die Option useFixedBase=True sorgt dafür, dass das Maze statisch bleibt.\n",
    "        self.mazeId = p.loadURDF(\"urdf/maze.urdf\",\n",
    "                                 basePosition=[0, 0, 0],\n",
    "                                 useFixedBase=True,\n",
    "                                 physicsClientId=self.client)\n",
    "\n",
    "        # Ziele definieren (2 verschiedene, beispielhaft)\n",
    "        # Passe sie an die tatsächlichen Gänge/Ausgänge in deinem Maze an.\n",
    "        self.goal_area_1 = np.array([4.5, 4.5])   # oberer Ausgang\n",
    "        self.goal_area_2 = np.array([4.5, -4.5])  # unterer Ausgang\n",
    "        self.goal_radius = 0.5\n",
    "        self.target_goal_pos = None  # Wird in reset() gesetzt\n",
    "        self.correct_goal_index = -1\n",
    "\n",
    "        # Falls im GUI-Modus: Visualisiere die Ziele als farbige Kugeln\n",
    "        if p.getConnectionInfo()['connectionMethod'] == p.GUI:\n",
    "            goal_visual_shape_1 = p.createVisualShape(\n",
    "                p.GEOM_SPHERE,\n",
    "                radius=self.goal_radius,\n",
    "                rgbaColor=[0, 1, 0, 0.5]  # halbdurchsichtig grün\n",
    "            )\n",
    "            goal_visual_shape_2 = p.createVisualShape(\n",
    "                p.GEOM_SPHERE,\n",
    "                radius=self.goal_radius,\n",
    "                rgbaColor=[1, 0, 0, 0.5]  # halbdurchsichtig rot\n",
    "            )\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_1,\n",
    "                              basePosition=[self.goal_area_1[0], self.goal_area_1[1], 0.1])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_2,\n",
    "                              basePosition=[self.goal_area_2[0], self.goal_area_2[1], 0.1])\n",
    "\n",
    "        # Roboter (Auto) laden\n",
    "        # Passe ggf. den Pfad an, wenn das URDF woanders liegt\n",
    "        self.start_pos = [-4.5, 0.0, 0.1]  # Start nahe der linken Seite des Maze\n",
    "        self.start_orn = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.carId = p.loadURDF(\"urdf/simple_two_wheel_car.urdf\",\n",
    "                                self.start_pos, self.start_orn,\n",
    "                                physicsClientId=self.client)\n",
    "        \n",
    "        # IDs der Räder (müssen zu deinem Roboter-URDF passen)\n",
    "        self.left_wheel_joint_index = 1\n",
    "        self.right_wheel_joint_index = 0\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.max_steps_per_episode = 100  # z. B. mehr Schritte erlauben\n",
    "\n",
    "        self.action_repeat = 100\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        euler = p.getEulerFromQuaternion(orn_quat)\n",
    "        yaw = euler[2]\n",
    "        return np.array([pos[0], pos[1], yaw,\n",
    "                         self.target_goal_pos[0],\n",
    "                         self.target_goal_pos[1]], dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        dist_goal1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1)\n",
    "        dist_goal2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2)\n",
    "        return {\n",
    "            \"distance_goal1\": dist_goal1,\n",
    "            \"distance_goal2\": dist_goal2,\n",
    "            \"target_goal_index\": self.correct_goal_index\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Auto zurücksetzen\n",
    "        p.resetBasePositionAndOrientation(self.carId, self.start_pos, self.start_orn, physicsClientId=self.client)\n",
    "        p.resetBaseVelocity(self.carId,\n",
    "                            linearVelocity=[0, 0, 0],\n",
    "                            angularVelocity=[0, 0, 0],\n",
    "                            physicsClientId=self.client)\n",
    "\n",
    "        # Zufällige Wahl des \"richtigen\" Ziels (oder fest vorgegeben)\n",
    "        # self.correct_goal_index = self.np_random.integers(0, 2)\n",
    "        self.correct_goal_index = 0  # zum Testen immer Goal 1\n",
    "        if self.correct_goal_index == 0:\n",
    "            self.target_goal_pos = self.goal_area_1\n",
    "        else:\n",
    "            self.target_goal_pos = self.goal_area_2\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "            # time.sleep(0.02)\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        if action == 0:\n",
    "            left_vel, right_vel = 10.0, 10.0\n",
    "        elif action == 1:\n",
    "            left_vel, right_vel = 3.0, 10.0\n",
    "        else:  # action == 2\n",
    "            left_vel, right_vel = 10.0, 3.0\n",
    "\n",
    "        # Setze die Motoren der beiden Räder\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.left_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=left_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.right_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=right_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "\n",
    "\n",
    "        # print(f\"Action: {action}, Velocities: L={left_vel}, R={right_vel}, Step={self.step_counter}\")\n",
    "        # car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        # print(f\"Position: {car_pos}\")\n",
    "\n",
    "\n",
    "        # Jetzt die Simulation mehrmals updaten,\n",
    "        # damit der Roboter tatsächlich fährt und gegen Wände kollidieren kann\n",
    "        for _ in range(self.action_repeat):\n",
    "            p.stepSimulation()\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "        # Beobachtung + Reward + Done bestimmen\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Check goal conditions\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        in_goal_1 = np.linalg.norm(car_pos[:2] - self.goal_area_1) < self.goal_radius\n",
    "        in_goal_2 = np.linalg.norm(car_pos[:2] - self.goal_area_2) < self.goal_radius\n",
    "\n",
    "        terminated = False\n",
    "        reward = -0.05  # kleiner Schritt-Penalty\n",
    "\n",
    "        if in_goal_1:\n",
    "            if self.correct_goal_index == 0:\n",
    "                reward = 10.0\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -5.0\n",
    "                terminated = True\n",
    "        elif in_goal_2:\n",
    "            if self.correct_goal_index == 1:\n",
    "                reward = 10.0\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -5.0\n",
    "                terminated = True\n",
    "\n",
    "        truncated = (self.step_counter >= self.max_steps_per_episode)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # Bei PyBullet im GUI-Modus passiert das Rendering automatisch\n",
    "        pass\n",
    "\n",
    "    def _render_frame(self):\n",
    "        # Debug-Anzeigen, falls erwünscht\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(physicsClientId=self.client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check passed!\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# env = MazeCarEnv(render_mode=None) \n",
    "env = MazeCarEnv(render_mode=None) \n",
    "\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "    env.close()\n",
    "    exit()\n",
    "\n",
    "# model = SAC(\n",
    "#     \"MlpPolicy\", \n",
    "#     env, \n",
    "#     verbose=1, \n",
    "#     tensorboard_log=\"./sac_mazecar_tensorboard/\",\n",
    "#     batch_size=64,\n",
    "#     learning_rate=0.0003,\n",
    "#     train_freq=1,\n",
    "#     gradient_steps=1,\n",
    "#     buffer_size=1000000,\n",
    "#     tau=0.005,\n",
    "#     gamma=0.99\n",
    "# )\n",
    "\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./ppo_mazecar_tensorboard/\",\n",
    "            batch_size=64,\n",
    "            learning_rate=0.0003,\n",
    "            n_steps=4096,\n",
    "            device=\"cuda\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    }
   ],
   "source": [
    "env = MazeCarEnv(render_mode=\"human\")  # statt None → GUI aktiv!\n",
    "obs, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for _ in range(200):\n",
    "    obs, reward, terminated, truncated, info = env.step(0)  # z. B. immer geradeaus\n",
    "    time.sleep(0.05)  # damit man die Bewegung sieht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Logging to ./ppo_mazecar_tensorboard/PPO_27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=2500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -5       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 175      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -5          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 5000        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012565255 |\n",
      "|    clip_fraction        | 0.0503      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.198      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0271      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00288    |\n",
      "|    value_loss           | 0.0519      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -5       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 160      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -5          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012731753 |\n",
      "|    clip_fraction        | 0.0237      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | 0.0357      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00918     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00146    |\n",
      "|    value_loss           | 0.0408      |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 100         |\n",
      "|    mean_reward          | -5          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12500       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010068846 |\n",
      "|    clip_fraction        | 0.0536      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | 0.046       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00251     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 0.0302      |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -5       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 157      |\n",
      "|    iterations      | 4        |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=17500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -5           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 17500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054356474 |\n",
      "|    clip_fraction        | 0.0226       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.654       |\n",
      "|    explained_variance   | 0.054        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0126       |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00102     |\n",
      "|    value_loss           | 0.0225       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -5       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=22500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -5           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 22500        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068722614 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.618       |\n",
      "|    explained_variance   | 0.0862       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000831     |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00137     |\n",
      "|    value_loss           | 0.0165       |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 159      |\n",
      "|    iterations      | 6        |\n",
      "|    time_elapsed    | 154      |\n",
      "|    total_timesteps | 24576    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 100          |\n",
      "|    mean_reward          | -5           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 25000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065970626 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.623       |\n",
      "|    explained_variance   | 0.195        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0127      |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 0.0116       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=27500, episode_reward=-5.00 +/- 0.00\n",
      "Episode length: 100.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 100      |\n",
      "|    mean_reward     | -5       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 27500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 100      |\n",
      "|    ep_rew_mean     | -5       |\n",
      "| time/              |          |\n",
      "|    fps             | 158      |\n",
      "|    iterations      | 7        |\n",
      "|    time_elapsed    | 180      |\n",
      "|    total_timesteps | 28672    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m      2\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: MazeCarEnv(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m      3\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(\n\u001b[1;32m      4\u001b[0m     eval_env, \n\u001b[1;32m      5\u001b[0m     best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./logs/\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# model.learn(total_timesteps=20000)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_mazecar_model_2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:222\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[2], line 155\u001b[0m, in \u001b[0;36mMazeCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# print(f\"Action: {action}, Velocities: L={left_vel}, R={right_vel}, Step={self.step_counter}\")\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# print(f\"Position: {car_pos}\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# Jetzt die Simulation mehrmals updaten,\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# damit der Roboter tatsächlich fährt und gegen Wände kollidieren kann\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_repeat):\n\u001b[0;32m--> 155\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstepSimulation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Beobachtung + Reward + Done bestimmen\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "eval_env = DummyVecEnv([lambda: MazeCarEnv(render_mode=\"human\")])\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env, \n",
    "    best_model_save_path='./logs/',\n",
    "    log_path='./logs/', \n",
    "    eval_freq=2500,\n",
    "    deterministic=True, \n",
    "    render=True\n",
    ")\n",
    "\n",
    "\n",
    "model.learn(total_timesteps=200000, callback=eval_callback)\n",
    "# model.learn(total_timesteps=20000)\n",
    "\n",
    "model.save(\"ppo_mazecar_model_2\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "destroy semaphore\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      9\u001b[0m  \u001b[38;5;66;03m# Convert action to the correct format if necessary\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# if isinstance(action, np.ndarray) and action.size == 1:  # Discrete action\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#     action = action.item()  # Convert to scalar (e.g., 2 instead of [2])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m240.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust sleep time for rendering speed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation episode finished. Reached target goal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_goal_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- To evaluate ---\n",
    "eval_env.close()\n",
    "eval_env = MazeCarEnv(render_mode=\"human\") # Render during evaluation\n",
    "model = PPO.load(\"ppo_mazecar_model_2\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "for _ in range(10000): # Max steps for evaluation\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "     # Convert action to the correct format if necessary\n",
    "    # if isinstance(action, np.ndarray) and action.size == 1:  # Discrete action\n",
    "    #     action = action.item()  # Convert to scalar (e.g., 2 instead of [2])\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    time.sleep(1./240.0)  # Adjust sleep time for rendering speed\n",
    "    if terminated or truncated:\n",
    "        print(f\"Evaluation episode finished. Reached target goal: {info.get('target_goal_index')}, Reward: {reward}\")\n",
    "        obs, info = eval_env.reset() # Reset for next evaluation episode\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n"
     ]
    }
   ],
   "source": [
    "eval_env = MazeCarEnv(render_mode=\"human\") # Render during evaluation\n",
    "\n",
    "p.setJointMotorControl2(env.carId, 0, p.VELOCITY_CONTROL, targetVelocity=10.0, force=30)\n",
    "p.setJointMotorControl2(env.carId, 1, p.VELOCITY_CONTROL, targetVelocity=10.0, force=30)\n",
    "\n",
    "for _ in range(500):\n",
    "    p.stepSimulation()\n",
    "    time.sleep(1./60.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/masterthesis/mt_start$   tensorboard --logdir=./ppo_mazecar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.get_device_name(0))  # Prints the name of the GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pybullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
