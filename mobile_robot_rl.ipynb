{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCarEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], \"render_fps\": 30}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Define Action Space (Discrete Example) ---\n",
    "        # 0: Forward, 1: Left, 2: Right\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # --- Define Observation Space ---\n",
    "        # Example: [car_x, car_y, car_yaw, target_x, target_y]\n",
    "        # Needs careful normalization/scaling! Box space is common.\n",
    "        # Define reasonable low/high bounds based on your expected maze size\n",
    "        low = np.array([-10, -10, -np.pi, -10, -10], dtype=np.float32)\n",
    "        high = np.array([10, 10, np.pi, 10, 10], dtype=np.float32)\n",
    "        self.observation_space = spaces.Box(low, high, dtype=np.float32)\n",
    "\n",
    "        # --- PyBullet Setup ---\n",
    "        self.render_mode = render_mode\n",
    "        self.client = p.connect(p.DIRECT if render_mode is None else p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81)\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
    "        # Define goal areas (use class attributes)\n",
    "        self.goal_area_1 = np.array([5, 5]) # Store only XY\n",
    "        self.goal_area_2 = np.array([5, -5])\n",
    "        self.goal_radius = 0.5\n",
    "        self.target_goal_pos = None # Will be set in reset()\n",
    "        self.correct_goal_index = -1 # 0 for goal 1, 1 for goal 2\n",
    "        # Visualize goals (only works in GUI mode)\n",
    "        if p.getConnectionInfo()['connectionMethod'] == p.GUI:\n",
    "            goal_visual_shape_1 = p.createVisualShape(p.GEOM_SPHERE, radius=self.goal_radius, rgbaColor=[0, 1, 0, 0.5])\n",
    "            goal_visual_shape_2 = p.createVisualShape(p.GEOM_SPHERE, radius=self.goal_radius, rgbaColor=[1, 0, 0, 0.5])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_1, basePosition=[5, 5, 0.1])\n",
    "            p.createMultiBody(baseVisualShapeIndex=goal_visual_shape_2, basePosition=[5, -5, 0.1])\n",
    "\n",
    "        # Load car (ensure path is correct)\n",
    "        self.start_pos = [0,0,0.1]\n",
    "        self.start_orn = p.getQuaternionFromEuler([0,0,0])\n",
    "        self.carId = p.loadURDF(\"/urdf/simple_two_wheel_car.urdf\", self.start_pos, self.start_orn, physicsClientId=self.client)\n",
    "        # FIND YOUR JOINT INDICES HERE (as done in Phase 1)\n",
    "        self.left_wheel_joint_index = 1 # Replace with actual index\n",
    "        self.right_wheel_joint_index = 0 # Replace with actual index\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.max_steps_per_episode = 80 # Limit episode length\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        euler = p.getEulerFromQuaternion(orn_quat)\n",
    "        yaw = euler[2]\n",
    "        # Return observation matching the defined space\n",
    "        return np.array([pos[0], pos[1], yaw, self.target_goal_pos[0], self.target_goal_pos[1]], dtype=np.float32)\n",
    "\n",
    "    def _get_info(self):\n",
    "        # Optional: provide extra info not used for learning\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        dist_goal1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1)\n",
    "        dist_goal2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2)\n",
    "        return {\"distance_goal1\": dist_goal1, \"distance_goal2\": dist_goal2, \"target_goal_index\": self.correct_goal_index}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Reset car position/orientation\n",
    "        p.resetBasePositionAndOrientation(self.carId, self.start_pos, self.start_orn, physicsClientId=self.client)\n",
    "        p.resetBaseVelocity(self.carId, linearVelocity=[0,0,0], angularVelocity=[0,0,0], physicsClientId=self.client)\n",
    "        # Reset wheel velocities (important!)\n",
    "\n",
    "        # --- Randomly choose the CORRECT goal for this episode ---\n",
    "        self.correct_goal_index = self.np_random.integers(0, 2) # 0 or 1\n",
    "        if self.correct_goal_index == 0:\n",
    "            self.target_goal_pos = self.goal_area_1\n",
    "        else:\n",
    "            self.target_goal_pos = self.goal_area_2\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Optional: Add goal visualization if rendering\n",
    "        if self.render_mode == \"human\":\n",
    "             self._render_frame() # Might need specific visualization logic here\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Get the current position and orientation of the robot\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        yaw = p.getEulerFromQuaternion(orn_quat)[2]  # Extract yaw (rotation around Z-axis)\n",
    "\n",
    "        # Define step size and turn angle\n",
    "        step_size = 0.5  # Distance to move forward\n",
    "        turn_angle = np.pi / 2  # 90 degrees in radians\n",
    "\n",
    "        # Update position and orientation based on the action\n",
    "        if action == 0:  # Move forward\n",
    "            new_pos = [pos[0] + step_size * np.cos(yaw), pos[1] + step_size * np.sin(yaw), pos[2]]\n",
    "            new_yaw = yaw\n",
    "        elif action == 1:  # Turn left\n",
    "            new_pos = pos  # Stay in the same position\n",
    "            new_yaw = yaw + turn_angle\n",
    "        elif action == 2:  # Turn right\n",
    "            new_pos = pos  # Stay in the same position\n",
    "            new_yaw = yaw - turn_angle\n",
    "\n",
    "        # Normalize yaw to keep it within [-pi, pi]\n",
    "        new_yaw = (new_yaw + np.pi) % (2 * np.pi) - np.pi\n",
    "\n",
    "        # Set the new position and orientation\n",
    "        new_orn = p.getQuaternionFromEuler([0, 0, new_yaw])\n",
    "        p.resetBasePositionAndOrientation(self.carId, new_pos, new_orn, physicsClientId=self.client)\n",
    "\n",
    "        self.step_counter += 1\n",
    "\n",
    "        # Get observation, calculate reward, check termination/truncation\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # Check goal conditions\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        in_goal_1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1) < self.goal_radius\n",
    "        in_goal_2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2) < self.goal_radius\n",
    "\n",
    "        terminated = False\n",
    "        reward = -0.1  # Small penalty per step\n",
    "\n",
    "        if in_goal_1:\n",
    "            if self.correct_goal_index == 0:\n",
    "                reward = 100.0  # Reached correct goal 1\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -50.0  # Reached incorrect goal 1\n",
    "                terminated = True\n",
    "        elif in_goal_2:\n",
    "            if self.correct_goal_index == 1:\n",
    "                reward = 100.0  # Reached correct goal 2\n",
    "                terminated = True\n",
    "            else:\n",
    "                reward = -50.0  # Reached incorrect goal 2\n",
    "                terminated = True\n",
    "\n",
    "        # Check truncation (max steps exceeded)\n",
    "        truncated = self.step_counter >= self.max_steps_per_episode\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # PyBullet handles rendering if connected with p.GUI\n",
    "        # This method might be needed for specific Gym requirements or offscreen rendering\n",
    "        if self.render_mode == \"human\":\n",
    "             # Usually PyBullet's GUI handles this, but you might add overlays here\n",
    "             pass\n",
    "        # Implement other modes like 'rgb_array' if needed\n",
    "        return None # Or an image array\n",
    "\n",
    "    def _render_frame(self):\n",
    "         # Could add debug text, lines etc. using p.addUserDebugText, p.addUserDebugLine\n",
    "         pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(physicsClientId=self.client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check passed!\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO # Or A2C, DQN for discrete actions\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "# env = MazeCarEnv(render_mode=None) \n",
    "env = MazeCarEnv(render_mode=None) \n",
    "\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment check passed!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "    env.close()\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Define the model (PPO is a good starting point for Box observations)\n",
    "model = PPO(\"MlpPolicy\", \n",
    "            env, \n",
    "            verbose=1, \n",
    "            tensorboard_log=\"./ppo_mazecar_tensorboard/\",\n",
    "            batch_size=64,\n",
    "            learning_rate=0.0003,\n",
    "            n_steps=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Logging to ./ppo_mazecar_tensorboard/PPO_14\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80       |\n",
      "|    ep_rew_mean     | -8       |\n",
      "| time/              |          |\n",
      "|    fps             | 693      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 80          |\n",
      "|    ep_rew_mean          | -8          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009418864 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.695       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0719      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00855    |\n",
      "|    value_loss           | 0.198       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ferdinand/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009063898 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.839       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0302      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0098     |\n",
      "|    value_loss           | 0.129       |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.8     |\n",
      "|    ep_rew_mean     | -8.48    |\n",
      "| time/              |          |\n",
      "|    fps             | 260      |\n",
      "|    iterations      | 3        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 12288    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78.8        |\n",
      "|    ep_rew_mean          | -7.88       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009615054 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0389      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.17        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00317    |\n",
      "|    value_loss           | 4.91        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065923384 |\n",
      "|    clip_fraction        | 0.013        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | -0.013       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 14.1         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00443     |\n",
      "|    value_loss           | 55.7         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.8     |\n",
      "|    ep_rew_mean     | -3.17    |\n",
      "| time/              |          |\n",
      "|    fps             | 234      |\n",
      "|    iterations      | 5        |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 20480    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 77.2         |\n",
      "|    ep_rew_mean          | -3.71        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 234          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055129244 |\n",
      "|    clip_fraction        | 0.00815      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.98        |\n",
      "|    explained_variance   | 0.136        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.5         |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 77.1         |\n",
      "|    ep_rew_mean          | -5.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 119          |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066187643 |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.96        |\n",
      "|    explained_variance   | 0.563        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 50.8         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00369     |\n",
      "|    value_loss           | 28.5         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-34.40 +/- 21.56\n",
      "Episode length: 44.60 +/- 28.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 44.6         |\n",
      "|    mean_reward          | -34.4        |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 30000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040886803 |\n",
      "|    clip_fraction        | 0.0103       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.937       |\n",
      "|    explained_variance   | 0.38         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 87.6         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00227     |\n",
      "|    value_loss           | 76.2         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78       |\n",
      "|    ep_rew_mean     | -6.79    |\n",
      "| time/              |          |\n",
      "|    fps             | 239      |\n",
      "|    iterations      | 8        |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 32768    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.3         |\n",
      "|    ep_rew_mean          | -0.122       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 230          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 159          |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070125014 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.94        |\n",
      "|    explained_variance   | 0.544        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 61.8         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0032      |\n",
      "|    value_loss           | 26.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 40000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050579705 |\n",
      "|    clip_fraction        | 0.0143       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.881       |\n",
      "|    explained_variance   | 0.289        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.8         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00156     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.6     |\n",
      "|    ep_rew_mean     | -0.148   |\n",
      "| time/              |          |\n",
      "|    fps             | 223      |\n",
      "|    iterations      | 10       |\n",
      "|    time_elapsed    | 183      |\n",
      "|    total_timesteps | 40960    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 74.9        |\n",
      "|    ep_rew_mean          | 4.52        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 221         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008518061 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.863      |\n",
      "|    explained_variance   | 0.47        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.5        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00273    |\n",
      "|    value_loss           | 68.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 74.2         |\n",
      "|    ep_rew_mean          | 5.1          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 225          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 217          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0029143682 |\n",
      "|    clip_fraction        | 0.00979      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.816       |\n",
      "|    explained_variance   | 0.403        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 62           |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 120          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 50000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004613132 |\n",
      "|    clip_fraction        | 0.0229      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.785      |\n",
      "|    explained_variance   | 0.488       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 113         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.7     |\n",
      "|    ep_rew_mean     | 0.943    |\n",
      "| time/              |          |\n",
      "|    fps             | 224      |\n",
      "|    iterations      | 13       |\n",
      "|    time_elapsed    | 237      |\n",
      "|    total_timesteps | 53248    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.3         |\n",
      "|    ep_rew_mean          | 0.378        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 221          |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 258          |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039307084 |\n",
      "|    clip_fraction        | 0.0155       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.791       |\n",
      "|    explained_variance   | 0.546        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 40.8         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00251     |\n",
      "|    value_loss           | 63.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 60000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049388832 |\n",
      "|    clip_fraction        | 0.0168       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.777       |\n",
      "|    explained_variance   | 0.446        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 79.4         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 73.8         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.8     |\n",
      "|    ep_rew_mean     | -0.671   |\n",
      "| time/              |          |\n",
      "|    fps             | 229      |\n",
      "|    iterations      | 15       |\n",
      "|    time_elapsed    | 268      |\n",
      "|    total_timesteps | 61440    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78          |\n",
      "|    ep_rew_mean          | -2.79       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 272         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002025547 |\n",
      "|    clip_fraction        | 0.0289      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.783      |\n",
      "|    explained_variance   | 0.346       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 84.6        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00192    |\n",
      "|    value_loss           | 61.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 79.1         |\n",
      "|    ep_rew_mean          | -5.91        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 251          |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 276          |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0128707625 |\n",
      "|    clip_fraction        | 0.0843       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.795       |\n",
      "|    explained_variance   | 0.257        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.672        |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00176     |\n",
      "|    value_loss           | 43.3         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=34.32 +/- 51.83\n",
      "Episode length: 57.20 +/- 27.92\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 57.2        |\n",
      "|    mean_reward          | 34.3        |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 70000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012073382 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.87       |\n",
      "|    explained_variance   | 0.332       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.17        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    value_loss           | 21.4        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.1     |\n",
      "|    ep_rew_mean     | -3.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 262      |\n",
      "|    iterations      | 18       |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 73728    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 79.1         |\n",
      "|    ep_rew_mean          | -2.91        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 270          |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 287          |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067234626 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.821       |\n",
      "|    explained_variance   | 0.202        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 58.7         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00372     |\n",
      "|    value_loss           | 55.9         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 80000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004103456 |\n",
      "|    clip_fraction        | 0.026       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.797      |\n",
      "|    explained_variance   | 0.208       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.08        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.000932   |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.3     |\n",
      "|    ep_rew_mean     | -3.92    |\n",
      "| time/              |          |\n",
      "|    fps             | 280      |\n",
      "|    iterations      | 20       |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 81920    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 78.4        |\n",
      "|    ep_rew_mean          | -1.83       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 298         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009715095 |\n",
      "|    clip_fraction        | 0.0837      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.737      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00779    |\n",
      "|    value_loss           | 38.9        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 90000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035522142 |\n",
      "|    clip_fraction        | 0.0212       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.725       |\n",
      "|    explained_variance   | 0.356        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.59         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 60.5         |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.5     |\n",
      "|    ep_rew_mean     | 1.36     |\n",
      "| time/              |          |\n",
      "|    fps             | 295      |\n",
      "|    iterations      | 22       |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 90112    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.5         |\n",
      "|    ep_rew_mean          | 1.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 304          |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 309          |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035491334 |\n",
      "|    clip_fraction        | 0.0214       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.713       |\n",
      "|    explained_variance   | 0.337        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 16           |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00207     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 75.8         |\n",
      "|    ep_rew_mean          | 6.43         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 316          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063062543 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.695       |\n",
      "|    explained_variance   | 0.411        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.32         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00229     |\n",
      "|    value_loss           | 72.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 100000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015838167 |\n",
      "|    clip_fraction        | 0.00762      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.672       |\n",
      "|    explained_variance   | 0.31         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 43.5         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.000764    |\n",
      "|    value_loss           | 149          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.9     |\n",
      "|    ep_rew_mean     | 7.32     |\n",
      "| time/              |          |\n",
      "|    fps             | 319      |\n",
      "|    iterations      | 25       |\n",
      "|    time_elapsed    | 320      |\n",
      "|    total_timesteps | 102400   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 76.5         |\n",
      "|    ep_rew_mean          | 5.36         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 327          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 325          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027762067 |\n",
      "|    clip_fraction        | 0.0235       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.693       |\n",
      "|    explained_variance   | 0.597        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 52.7         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    value_loss           | 74.7         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 110000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004388611 |\n",
      "|    clip_fraction        | 0.0671      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.711      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.3        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0042     |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.5     |\n",
      "|    ep_rew_mean     | 7.46     |\n",
      "| time/              |          |\n",
      "|    fps             | 332      |\n",
      "|    iterations      | 27       |\n",
      "|    time_elapsed    | 332      |\n",
      "|    total_timesteps | 110592   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 76.9        |\n",
      "|    ep_rew_mean          | 4.32        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 340         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005371381 |\n",
      "|    clip_fraction        | 0.0367      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.639      |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 87.5        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00336    |\n",
      "|    value_loss           | 140         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 77.7        |\n",
      "|    ep_rew_mean          | -0.762      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 343         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010215709 |\n",
      "|    clip_fraction        | 0.0751      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.569       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00115    |\n",
      "|    value_loss           | 61.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 120000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003934282 |\n",
      "|    clip_fraction        | 0.0365      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.59        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 84.5        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.3     |\n",
      "|    ep_rew_mean     | 7.48     |\n",
      "| time/              |          |\n",
      "|    fps             | 353      |\n",
      "|    iterations      | 30       |\n",
      "|    time_elapsed    | 347      |\n",
      "|    total_timesteps | 122880   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 74.1        |\n",
      "|    ep_rew_mean          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 362         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003651423 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.622      |\n",
      "|    explained_variance   | 0.417       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 138         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0027     |\n",
      "|    value_loss           | 155         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=13.10 +/- 42.20\n",
      "Episode length: 69.20 +/- 21.60\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 69.2         |\n",
      "|    mean_reward          | 13.1         |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 130000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038471292 |\n",
      "|    clip_fraction        | 0.0457       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.524        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 21.9         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.003       |\n",
      "|    value_loss           | 153          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.1     |\n",
      "|    ep_rew_mean     | 13.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 370      |\n",
      "|    iterations      | 32       |\n",
      "|    time_elapsed    | 353      |\n",
      "|    total_timesteps | 131072   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 74.1        |\n",
      "|    ep_rew_mean          | 14.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 379         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005200308 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.608      |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.7        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 73           |\n",
      "|    ep_rew_mean          | 13.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 386          |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 360          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044436706 |\n",
      "|    clip_fraction        | 0.0527       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.509       |\n",
      "|    explained_variance   | 0.688        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.1         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.0046      |\n",
      "|    value_loss           | 124          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 140000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033367956 |\n",
      "|    clip_fraction        | 0.0272       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.476       |\n",
      "|    explained_variance   | 0.738        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.8         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00274     |\n",
      "|    value_loss           | 107          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.3     |\n",
      "|    ep_rew_mean     | 16.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 394      |\n",
      "|    iterations      | 35       |\n",
      "|    time_elapsed    | 362      |\n",
      "|    total_timesteps | 143360   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 71.6         |\n",
      "|    ep_rew_mean          | 16.9         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 402          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 365          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027833923 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.488       |\n",
      "|    explained_variance   | 0.734        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.8         |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 134          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 150000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038206603 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.464       |\n",
      "|    explained_variance   | 0.769        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.6         |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    value_loss           | 118          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.3     |\n",
      "|    ep_rew_mean     | 16.7     |\n",
      "| time/              |          |\n",
      "|    fps             | 410      |\n",
      "|    iterations      | 37       |\n",
      "|    time_elapsed    | 369      |\n",
      "|    total_timesteps | 151552   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 72.3         |\n",
      "|    ep_rew_mean          | 22.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 418          |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 372          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050621275 |\n",
      "|    clip_fraction        | 0.0419       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.475       |\n",
      "|    explained_variance   | 0.792        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 89.5         |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.3        |\n",
      "|    ep_rew_mean          | 25.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 425         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 375         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002501564 |\n",
      "|    clip_fraction        | 0.0316      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.516      |\n",
      "|    explained_variance   | 0.805       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0014     |\n",
      "|    value_loss           | 147         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "----------------------------------------\n",
      "| eval/                   |            |\n",
      "|    mean_ep_length       | 80         |\n",
      "|    mean_reward          | -8         |\n",
      "| time/                   |            |\n",
      "|    total_timesteps      | 160000     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00411087 |\n",
      "|    clip_fraction        | 0.047      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.482     |\n",
      "|    explained_variance   | 0.843      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 49.2       |\n",
      "|    n_updates            | 390        |\n",
      "|    policy_gradient_loss | -0.00104   |\n",
      "|    value_loss           | 122        |\n",
      "----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.4     |\n",
      "|    ep_rew_mean     | 17.8     |\n",
      "| time/              |          |\n",
      "|    fps             | 433      |\n",
      "|    iterations      | 40       |\n",
      "|    time_elapsed    | 378      |\n",
      "|    total_timesteps | 163840   |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.4        |\n",
      "|    ep_rew_mean          | 21.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 380         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002308642 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.81        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 53.2        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00075    |\n",
      "|    value_loss           | 103         |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 170000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0016970383 |\n",
      "|    clip_fraction        | 0.0195       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.503       |\n",
      "|    explained_variance   | 0.772        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 90.7         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.000671    |\n",
      "|    value_loss           | 171          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.2     |\n",
      "|    ep_rew_mean     | 30       |\n",
      "| time/              |          |\n",
      "|    fps             | 445      |\n",
      "|    iterations      | 42       |\n",
      "|    time_elapsed    | 385      |\n",
      "|    total_timesteps | 172032   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 72.3         |\n",
      "|    ep_rew_mean          | 18.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 452          |\n",
      "|    iterations           | 43           |\n",
      "|    time_elapsed         | 388          |\n",
      "|    total_timesteps      | 176128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073416186 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.52        |\n",
      "|    explained_variance   | 0.829        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.8         |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.00228     |\n",
      "|    value_loss           | 129          |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 180000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004176804 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.903       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 55.8        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.000831   |\n",
      "|    value_loss           | 67.9        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.4     |\n",
      "|    ep_rew_mean     | 22.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 457      |\n",
      "|    iterations      | 44       |\n",
      "|    time_elapsed    | 393      |\n",
      "|    total_timesteps | 180224   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 70.5         |\n",
      "|    ep_rew_mean          | 23           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 464          |\n",
      "|    iterations           | 45           |\n",
      "|    time_elapsed         | 396          |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076434286 |\n",
      "|    clip_fraction        | 0.0344       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.49        |\n",
      "|    explained_variance   | 0.886        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.6         |\n",
      "|    n_updates            | 440          |\n",
      "|    policy_gradient_loss | -0.00142     |\n",
      "|    value_loss           | 89.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 71.3        |\n",
      "|    ep_rew_mean          | 23.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 469         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002555072 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.492      |\n",
      "|    explained_variance   | 0.91        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 44          |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000411   |\n",
      "|    value_loss           | 88.5        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 80           |\n",
      "|    mean_reward          | -8           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 190000       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027597426 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.485       |\n",
      "|    explained_variance   | 0.896        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35.1         |\n",
      "|    n_updates            | 460          |\n",
      "|    policy_gradient_loss | 0.000404     |\n",
      "|    value_loss           | 103          |\n",
      "------------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.2     |\n",
      "|    ep_rew_mean     | 28.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 473      |\n",
      "|    iterations      | 47       |\n",
      "|    time_elapsed    | 406      |\n",
      "|    total_timesteps | 192512   |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 67.8         |\n",
      "|    ep_rew_mean          | 25.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 480          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 409          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043787723 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.935        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37           |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 52.2         |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=-8.00 +/- 0.00\n",
      "Episode length: 80.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 80          |\n",
      "|    mean_reward          | -8          |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 200000      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003712614 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.454      |\n",
      "|    explained_variance   | 0.925       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.4        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.000785   |\n",
      "|    value_loss           | 60.6        |\n",
      "-----------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.3     |\n",
      "|    ep_rew_mean     | 31.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 486      |\n",
      "|    iterations      | 49       |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 200704   |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "eval_env = DummyVecEnv([lambda: MazeCarEnv(render_mode=\"Human\")])\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=10000,\n",
    "                             deterministic=True, render=True)\n",
    "model.learn(total_timesteps=200000, callback=eval_callback)\n",
    "# model.learn(total_timesteps=20000)\n",
    "model.save(\"ppo_mazecar_model\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=Intel\n",
      "GL_RENDERER=Mesa Intel(R) Graphics (RPL-P)\n",
      "GL_VERSION=4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "GL_SHADING_LANGUAGE_VERSION=4.60\n",
      "pthread_getconcurrency()=0\n",
      "Version = 4.6 (Core Profile) Mesa 23.2.1-1ubuntu3.1~22.04.3\n",
      "Vendor = Intel\n",
      "Renderer = Mesa Intel(R) Graphics (RPL-P)\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "ven = Intel\n",
      "Workaround for some crash in the Intel OpenGL driver on Linux/Ubuntu\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 0, Reward: -0.1\n",
      "Evaluation episode finished. Reached target goal: 1, Reward: -0.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Convert to scalar (e.g., 2 instead of [2])\u001b[39;00m\n\u001b[1;32m     13\u001b[0m obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m---> 14\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m240.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjust sleep time for rendering speed\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation episode finished. Reached target goal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_goal_index\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- To evaluate ---\n",
    "# eval_env.close()\n",
    "eval_env = MazeCarEnv(render_mode=\"human\") # Render during evaluation\n",
    "model = PPO.load(\"ppo_mazecar_model\", env=eval_env)\n",
    "\n",
    "obs, info = eval_env.reset()\n",
    "for _ in range(10000): # Max steps for evaluation\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "     # Convert action to the correct format if necessary\n",
    "    if isinstance(action, np.ndarray) and action.size == 1:  # Discrete action\n",
    "        action = action.item()  # Convert to scalar (e.g., 2 instead of [2])\n",
    "    \n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    time.sleep(1./240.0)  # Adjust sleep time for rendering speed\n",
    "    if terminated or truncated:\n",
    "        print(f\"Evaluation episode finished. Reached target goal: {info.get('target_goal_index')}, Reward: {reward}\")\n",
    "        obs, info = eval_env.reset() # Reset for next evaluation episode\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/masterthesis/mt_start$   tensorboard --logdir=./ppo_mazecar_tensorboard/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve_pybullet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
