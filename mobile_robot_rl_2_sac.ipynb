{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "from stable_baselines3 import SAC \n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecMonitor\n",
    "\n",
    "import time\n",
    "from stable_baselines3.common.callbacks import ProgressBarCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeCarEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human'], \"render_fps\": 500}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # --- Define Action Space ---\n",
    "        # Continuous actions: [left_motor_speed, right_motor_speed] ∈ [-1.0, 1.0]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0], dtype=np.float32),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # --- Define Observation Space ---\n",
    "        # Beobachtung: [Auto_x, Auto_y, Auto_yaw, Ziel_x, Ziel_y]\n",
    "        low = np.array([-6, -6, -np.pi, -6, -6], dtype=np.float32)\n",
    "        high = np.array([6, 6, np.pi, 6, 6], dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"state\": spaces.Box(\n",
    "                low=np.array([-6, -6, -np.pi, -6, -6], dtype=np.float32),\n",
    "                high=np.array([6, 6, np.pi, 6, 6], dtype=np.float32),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            \"camera\": spaces.Box(\n",
    "                low=0,\n",
    "                high=255,\n",
    "                shape=(64, 64, 4),  # RGBA Bild vom PyBullet\n",
    "                dtype=np.uint8\n",
    "            )\n",
    "        })\n",
    "\n",
    "        # --- PyBullet Setup ---\n",
    "        self.render_mode = render_mode\n",
    "        self.client = p.connect(p.DIRECT if render_mode is None else p.GUI)\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        p.setGravity(0, 0, -9.81, physicsClientId=self.client)\n",
    "        p.setTimeStep(1.0 / 240.0, physicsClientId=self.client)  # Standard 240 Hz\n",
    "\n",
    "        self.planeId = p.loadURDF(\"plane.urdf\", physicsClientId=self.client)\n",
    "\n",
    "        self.mazeId = p.loadURDF(\"urdf/maze_colored.urdf\",\n",
    "                                 basePosition=[0, 0, 0],\n",
    "                                 useFixedBase=True, # keeps maze static\n",
    "                                 physicsClientId=self.client)\n",
    "\n",
    "\n",
    "        self.goal_area_1 = np.array([-5.5, 4.5])   # oberer Ausgang\n",
    "        self.goal_area_2 = np.array([5.5, -4.5])  # unterer Ausgang\n",
    "        self.goal_radius = 0.5\n",
    "        self.target_goal_pos = None  # Wird in reset() gesetzt\n",
    "        self.correct_goal_index = None  # Zufällige Wahl des Ziels\n",
    "\n",
    "        self.goal_spheres = []\n",
    "\n",
    "        # Roboter (Auto) laden\n",
    "        self.start_pos = [-1.0, -2.0, 0.1] \n",
    "        self.start_orn = p.getQuaternionFromEuler([0, 0, 0])\n",
    "        self.carId = p.loadURDF(\"urdf/simple_two_wheel_car.urdf\",\n",
    "                                self.start_pos, self.start_orn,\n",
    "                                physicsClientId=self.client)\n",
    "        \n",
    "        self.left_wheel_joint_index = 1\n",
    "        self.right_wheel_joint_index = 0\n",
    "\n",
    "        self.step_counter = 0\n",
    "        self.max_steps_per_episode = 480\n",
    "\n",
    "        self.action_repeat = 50\n",
    "\n",
    "        self.trajectory = []\n",
    "        self.all_trajectories = []\n",
    "\n",
    "        self.was_in_goal = False \n",
    "        self.stopped = False\n",
    "\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        pos, orn_quat = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        euler = p.getEulerFromQuaternion(orn_quat)\n",
    "        yaw = euler[2]\n",
    "\n",
    "        camera_image = self._get_camera_image()\n",
    "\n",
    "        return {\n",
    "            \"state\": np.array([pos[0], pos[1], yaw, self.target_goal_pos[0], self.target_goal_pos[1]], dtype=np.float32),\n",
    "            \"camera\": camera_image\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_info(self):\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        dist_goal1 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_1)\n",
    "        dist_goal2 = np.linalg.norm(np.array(car_pos[:2]) - self.goal_area_2)\n",
    "\n",
    "        return {\n",
    "            \"distance_goal1\": dist_goal1,\n",
    "            \"distance_goal2\": dist_goal2,\n",
    "            \"target_goal_index\": self.correct_goal_index\n",
    "        }\n",
    "\n",
    "    def reset(self, seed=None, options=None, custom_goal_pos=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.step_counter = 0\n",
    "\n",
    "        # Auto zurücksetzen\n",
    "        start_x = self.start_pos[0] + self.np_random.uniform(-0.3, 0.3)\n",
    "        start_y = self.start_pos[1] + self.np_random.uniform(-0.3, 0.3)\n",
    "        start_yaw = self.np_random.uniform(-np.pi/6, np.pi/6)\n",
    "        start_orn = p.getQuaternionFromEuler([0, 0, start_yaw])\n",
    "        p.resetBasePositionAndOrientation(self.carId, [start_x, start_y, self.start_pos[2]], start_orn, physicsClientId=self.client)\n",
    "        p.resetBaseVelocity(self.carId,\n",
    "                            linearVelocity=[0, 0, 0],\n",
    "                            angularVelocity=[0, 0, 0],\n",
    "                            physicsClientId=self.client)\n",
    "\n",
    "        # Entferne bestehende Ziel-Sphären\n",
    "        for sphere_id in self.goal_spheres:\n",
    "            p.removeBody(sphere_id, physicsClientId=self.client)\n",
    "        self.goal_spheres.clear()\n",
    "\n",
    "\n",
    "        goal_visual_shape = p.createVisualShape(\n",
    "            p.GEOM_SPHERE,\n",
    "            radius=self.goal_radius,\n",
    "            rgbaColor=[0, 1, 0, 0.8]  # Bright green\n",
    "        )\n",
    "        \n",
    "         # Set the goal position\n",
    "        if custom_goal_pos is not None:\n",
    "            self.target_goal_pos = np.array(custom_goal_pos)\n",
    "            self.correct_goal_index = -1  # Custom goal, no correct index\n",
    "        else:\n",
    "            self.correct_goal_index = self.np_random.integers(0, 2)\n",
    "            self.target_goal_pos = self.goal_area_1 if self.correct_goal_index == 0 else self.goal_area_2\n",
    "        \n",
    "\n",
    "        # if self.correct_goal_index == 0:\n",
    "        #     self.target_goal_pos = self.goal_area_1\n",
    "            \n",
    "        #     if self.render_mode == \"human\":\n",
    "        #         goal_id = p.createMultiBody(\n",
    "        #             baseVisualShapeIndex=goal_visual_shape,\n",
    "        #             basePosition=[self.goal_area_1[0], self.goal_area_1[1], 0.1],\n",
    "        #             useMaximalCoordinates=True\n",
    "        #         )\n",
    "        #         self.goal_spheres.append(goal_id)\n",
    "        # else:\n",
    "        #     self.target_goal_pos = self.goal_area_2\n",
    "            \n",
    "        #     if self.render_mode == \"human\":\n",
    "        #         goal_id = p.createMultiBody(\n",
    "        #             baseVisualShapeIndex=goal_visual_shape,\n",
    "        #             basePosition=[self.goal_area_2[0], self.goal_area_2[1], 0.1],\n",
    "        #             useMaximalCoordinates=True\n",
    "        #         )\n",
    "        #         self.goal_spheres.append(goal_id)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "        \n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        self.prev_dist_to_goal = np.linalg.norm(np.array(car_pos[:2]) - self.target_goal_pos)\n",
    "\n",
    "        if len(self.trajectory) > 0:\n",
    "            self.all_trajectories.append(self.trajectory)  # Save old episode\n",
    "\n",
    "        self.trajectory = []\n",
    "        self.trajectory.append(car_pos[:2])  # only x, y\n",
    "\n",
    "        self.was_in_goal = False  \n",
    "        self.stopped = False\n",
    "\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # Clip the action to make sure it's in [-1, 1] range\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "\n",
    "        # Scale action to desired motor speeds\n",
    "        max_motor_speed = 10.0  # you can adjust this if needed\n",
    "        left_vel = float(action[0] * max_motor_speed)\n",
    "        right_vel = float(action[1] * max_motor_speed)\n",
    "\n",
    "        # left_vel += np.random.normal(0, 0.2) \n",
    "        # right_vel += np.random.normal(0, 0.2)\n",
    "\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.left_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=left_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "        p.setJointMotorControl2(\n",
    "            bodyUniqueId=self.carId,\n",
    "            jointIndex=self.right_wheel_joint_index,\n",
    "            controlMode=p.VELOCITY_CONTROL,\n",
    "            targetVelocity=right_vel,\n",
    "            force=20.0\n",
    "        )\n",
    "\n",
    "        # Jetzt die Simulation mehrmals updaten,\n",
    "        for _ in range(self.action_repeat):\n",
    "            p.stepSimulation()\n",
    "        \n",
    "        self.step_counter += 1\n",
    "\n",
    "        # Beobachtung + Reward + Done bestimmen\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        terminated = False\n",
    "\n",
    "        reward = 0.0\n",
    "        reward -= 0.005\n",
    "\n",
    "        # Calculate distance to the goal\n",
    "        car_pos, _ = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        curr_dist_to_goal = np.linalg.norm(np.array(car_pos[:2]) - self.target_goal_pos)\n",
    "\n",
    "        # Reward for getting closer to the goal\n",
    "        reward = 1 * (self.prev_dist_to_goal - curr_dist_to_goal)  # Positive reward for reducing distance\n",
    "        self.prev_dist_to_goal = curr_dist_to_goal\n",
    "\n",
    "        # Check if the car is in the goal area\n",
    "        in_goal_1 = np.linalg.norm(car_pos[:2] - self.goal_area_1) < self.goal_radius\n",
    "        in_goal_2 = np.linalg.norm(car_pos[:2] - self.goal_area_2) < self.goal_radius\n",
    "        in_goal = np.linalg.norm(car_pos[:2] - self.target_goal_pos) < self.goal_radius\n",
    "\n",
    "\n",
    "        # Check if the car is stationary in the goal area\n",
    "        linear_velocity, angular_velocity = p.getBaseVelocity(self.carId, physicsClientId=self.client)\n",
    "        speed = np.linalg.norm(linear_velocity)  # Calculate the speed from linear velocity\n",
    "\n",
    "                \n",
    "        if in_goal_1:\n",
    "            if self.correct_goal_index == 0:\n",
    "                if not self.was_in_goal:\n",
    "                    reward += 5.0\n",
    "                self.was_in_goal = True\n",
    "                if speed < 0.05 and self.stopped == False:  # Threshold to consider the car stationary\n",
    "                    reward = +3.0\n",
    "                    self.stopped = True\n",
    "                    terminated = True\n",
    "            else:\n",
    "                reward += -5.0\n",
    "                terminated = True\n",
    "                self.was_in_goal = True\n",
    "        elif in_goal_2:\n",
    "            if self.correct_goal_index == 1:\n",
    "                if not self.was_in_goal:\n",
    "                    reward += 5.0\n",
    "                self.was_in_goal = True\n",
    "                if speed < 0.05 and self.stopped == False:  # Threshold to consider the car stationary\n",
    "                    reward = +3.0\n",
    "                    self.stopped = True\n",
    "                    terminated = True\n",
    "            else:\n",
    "                reward += -5.0\n",
    "                terminated = True\n",
    "                self.was_in_goal = True\n",
    "        elif in_goal:\n",
    "            if self.correct_goal_index == -1:\n",
    "                if not self.was_in_goal:\n",
    "                    reward += 5.0\n",
    "                self.was_in_goal = True\n",
    "                if speed < 0.05 and self.stopped == False:  # Threshold to consider the car stationary\n",
    "                    reward = +3.0\n",
    "                    self.stopped = True\n",
    "                    terminated = True\n",
    "                else:\n",
    "                    reward += -5.0\n",
    "                    terminated = True\n",
    "                    self.was_in_goal = True\n",
    "\n",
    "  \n",
    "\n",
    "        self.trajectory.append(car_pos[:2])\n",
    "\n",
    "        #print(self.prev_dist_to_goal)\n",
    "\n",
    "\n",
    "        contacts = p.getContactPoints(bodyA=self.carId, bodyB=self.mazeId, physicsClientId=self.client)\n",
    "        if len(contacts) > 0:\n",
    "            reward -= 3.0\n",
    "            terminated = True \n",
    "\n",
    "        truncated = (self.step_counter >= self.max_steps_per_episode)\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self):\n",
    "        # Bei PyBullet im GUI-Modus passiert das Rendering automatisch\n",
    "        pass\n",
    "\n",
    "    def _render_frame(self):\n",
    "        # Debug-Anzeigen, falls erwünscht\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect(physicsClientId=self.client)\n",
    "\n",
    "    def _get_camera_image(self):\n",
    "        car_pos, car_orn = p.getBasePositionAndOrientation(self.carId, physicsClientId=self.client)\n",
    "        car_euler = p.getEulerFromQuaternion(car_orn)\n",
    "\n",
    "        camera_distance = 0.1\n",
    "        camera_height = 0.2\n",
    "        camera_yaw = car_euler[2] * 180 / np.pi\n",
    "        target_pos = [\n",
    "            car_pos[0] + camera_distance * np.cos(car_euler[2]),\n",
    "            car_pos[1] + camera_distance * np.sin(car_euler[2]),\n",
    "            car_pos[2] + camera_height\n",
    "        ]\n",
    "\n",
    "        width, height, rgba, _, _ = p.getCameraImage(\n",
    "            width=64,\n",
    "            height=64,\n",
    "            viewMatrix=p.computeViewMatrix(\n",
    "                cameraEyePosition=[car_pos[0], car_pos[1], car_pos[2] + camera_height],\n",
    "                cameraTargetPosition=target_pos,\n",
    "                cameraUpVector=[0, 0, 1]\n",
    "            ),\n",
    "            projectionMatrix=p.computeProjectionMatrixFOV(\n",
    "                fov=70,\n",
    "                aspect=1.0,\n",
    "                nearVal=0.01,\n",
    "                farVal=10.0\n",
    "            ),\n",
    "            physicsClientId=self.client\n",
    "        )\n",
    "\n",
    "        rgba_image = np.reshape(rgba, (height, width, 4)).astype(np.uint8)\n",
    "        return rgba_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(render_mode=None):\n",
    "    def _init():\n",
    "        return MazeCarEnv(render_mode=render_mode)\n",
    "    return _init\n",
    "\n",
    "num_envs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 550.120\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 550.120\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jemaritos/ferdinand_test/mt_start/env_f_mt_start/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:462: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf. https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment check passed!\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "# env = MazeCarEnv(render_mode=None) \n",
    "env = MazeCarEnv(render_mode=\"human\")\n",
    "\n",
    "p.resetDebugVisualizerCamera(cameraDistance=10, cameraYaw=-0.6, cameraPitch=-85, cameraTargetPosition=[0, 0, 0])\n",
    "\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment check passed!\")\n",
    "    env.close()\n",
    "except Exception as e:\n",
    "    print(f\"Environment check failed: {e}\")\n",
    "    env.close()\n",
    "    exit()\n",
    "\n",
    "env = SubprocVecEnv([make_env(render_mode=None) for _ in range(num_envs)])\n",
    "\n",
    "env = VecMonitor(env, \"./ppo_mazecar_tensorboard/\")  # Log to the specified directory\n",
    "\n",
    "model = SAC(\"MultiInputPolicy\", \n",
    "            env, \n",
    "            ent_coef=0.15,\n",
    "            gamma=0.97,\n",
    "            # verbose=1, \n",
    "            tensorboard_log=\"./sac_mazecar_tensorboard/\",\n",
    "            batch_size=256, # after n_steps the data is split into batches\n",
    "            learning_rate=0.0003,\n",
    "            train_freq=(512, \"step\"),  # how often to train\n",
    "            gradient_steps=512,        # steps per update\n",
    "            buffer_size=1_000_000,     # Wichtiger, damit nichts verloren geht\n",
    "            device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1224f18c8bf4f9993e56260a4e155ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# eval_env = MazeCarEnv(render_mode=\"human\")\n",
    "# eval_freq = 2000\n",
    "# eval_callback = EvalCallback(eval_env, eval_freq=eval_freq, verbose=1)\n",
    "\n",
    "model.learn(total_timesteps=650_000, progress_bar=True)\n",
    "\n",
    "model.save(\"models/sac_mazecar_model_2\")\n",
    "\n",
    "env.close()\n",
    "# check_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenPipeError",
     "evalue": "[Errno 32] Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_trajectories.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39marray(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall_trajectories\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mobject\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m all_trajectories \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_trajectories.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/ferdinand_test/mt_start/env_f_mt_start/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:414\u001b[0m, in \u001b[0;36mVecEnvWrapper.get_attr\u001b[0;34m(self, attr_name, indices)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_attr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name: \u001b[38;5;28mstr\u001b[39m, indices: VecEnvIndices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_attr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/ferdinand_test/mt_start/env_f_mt_start/lib/python3.10/site-packages/stable_baselines3/common/vec_env/subproc_vec_env.py:187\u001b[0m, in \u001b[0;36mSubprocVecEnv.get_attr\u001b[0;34m(self, attr_name, indices)\u001b[0m\n\u001b[1;32m    185\u001b[0m target_remotes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_target_remotes(indices)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m target_remotes:\n\u001b[0;32m--> 187\u001b[0m     \u001b[43mremote\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mget_attr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [remote\u001b[38;5;241m.\u001b[39mrecv() \u001b[38;5;28;01mfor\u001b[39;00m remote \u001b[38;5;129;01min\u001b[39;00m target_remotes]\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;66;03m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;66;03m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;66;03m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     remaining \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
     ]
    }
   ],
   "source": [
    "np.save(\"all_trajectories.npy\", np.array(env.get_attr(\"all_trajectories\")[0], dtype=object))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_trajectories = np.load(\"all_trajectories.npy\", allow_pickle=True)\n",
    "\n",
    "for ep_traj in all_trajectories:\n",
    "    plt.plot(np.array(ep_traj)[:,0], np.array(ep_traj)[:,1])\n",
    "\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Y\")\n",
    "plt.title(\"All Trajectories over Training\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "argc=2\n",
      "argv[0] = --unused\n",
      "argv[1] = --start_demo_name=Physics Server\n",
      "ExampleBrowserThreadFunc started\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
      "Creating context\n",
      "Created GL 3.3 context\n",
      "Direct GLX rendering context obtained\n",
      "Making context current\n",
      "GL_VENDOR=NVIDIA Corporation\n",
      "GL_RENDERER=NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "GL_VERSION=3.3.0 NVIDIA 550.120\n",
      "GL_SHADING_LANGUAGE_VERSION=3.30 NVIDIA via Cg compiler\n",
      "pthread_getconcurrency()=0\n",
      "Version = 3.3.0 NVIDIA 550.120\n",
      "Vendor = NVIDIA Corporation\n",
      "Renderer = NVIDIA GeForce RTX 4080 SUPER/PCIe/SSE2\n",
      "b3Printf: Selected demo: Physics Server\n",
      "startThreads creating 1 threads.\n",
      "starting thread 0\n",
      "started thread 0 \n",
      "MotionThreadFunc thread started\n",
      "ven = NVIDIA Corporation\n",
      "ven = NVIDIA Corporation\n",
      ".......................\n",
      ".......................\n",
      "Cumulated Reward: 0.020324006019944996\n",
      "Cumulated Reward: 0.037977046706417106\n",
      "Cumulated Reward: 0.05765817482961921\n",
      "Cumulated Reward: 0.09711072375416041\n",
      "Cumulated Reward: 0.1275801987517191\n",
      "Cumulated Reward: 0.1756217681051453\n",
      "Cumulated Reward: 0.23158504773820354\n",
      "Cumulated Reward: 0.25922173518334635\n",
      "Cumulated Reward: 0.30399878111964806\n",
      "Cumulated Reward: 0.35588754969289393\n",
      "Cumulated Reward: 0.4151841226009836\n",
      "Cumulated Reward: 0.4692270017642848\n",
      "Cumulated Reward: 0.5019855184066566\n",
      "Cumulated Reward: 0.556281938819791\n",
      "Cumulated Reward: 0.5985677911598124\n",
      "Cumulated Reward: 0.6424838846133616\n",
      "Cumulated Reward: 0.7075889808155909\n",
      "Cumulated Reward: 0.7610942581456315\n",
      "Cumulated Reward: 0.8026784400392613\n",
      "Cumulated Reward: 0.8525477649657112\n",
      "Cumulated Reward: 0.9155316847386272\n",
      "Cumulated Reward: 0.9493645430879627\n",
      "Cumulated Reward: 0.9725530646489604\n",
      "Cumulated Reward: 1.0317219264760364\n",
      "Cumulated Reward: 1.0892113842516542\n",
      "Cumulated Reward: 1.1259412435010616\n",
      "Cumulated Reward: 1.1925978530872063\n",
      "Cumulated Reward: 1.2675052721538878\n",
      "Cumulated Reward: 1.3211998069708102\n",
      "Cumulated Reward: 1.3953354206048276\n",
      "Cumulated Reward: 1.4769243755731911\n",
      "Cumulated Reward: 1.5734153076888577\n",
      "Cumulated Reward: 1.6384733964699376\n",
      "Cumulated Reward: 1.673351582025572\n",
      "Cumulated Reward: 1.7228857818344774\n",
      "Cumulated Reward: 1.7530790600253918\n",
      "Cumulated Reward: 1.830487589127788\n",
      "Cumulated Reward: 1.8899593147307576\n",
      "Cumulated Reward: 1.943289541199674\n",
      "Cumulated Reward: 1.9732694368035877\n",
      "Cumulated Reward: 2.030695108642565\n",
      "Cumulated Reward: 2.097861186197309\n",
      "Cumulated Reward: 2.1318361896000457\n",
      "Cumulated Reward: 2.1847548732290205\n",
      "Cumulated Reward: 2.265958357306287\n",
      "Cumulated Reward: 2.3115515912281883\n",
      "Cumulated Reward: 2.3880669835335495\n",
      "Cumulated Reward: 2.44488784720147\n",
      "Cumulated Reward: 2.5020675437915285\n",
      "Cumulated Reward: 2.556171013754204\n",
      "Cumulated Reward: 2.588135134115522\n",
      "Cumulated Reward: 2.652799323772048\n",
      "Cumulated Reward: 2.7144575707326553\n",
      "Cumulated Reward: 2.7961157262119265\n",
      "Cumulated Reward: 2.8328989400185147\n",
      "Cumulated Reward: 2.865873650863858\n",
      "Cumulated Reward: 2.9172520173845697\n",
      "Cumulated Reward: 2.9820524567681375\n",
      "Cumulated Reward: 3.0274023677486595\n",
      "Cumulated Reward: 3.082983157556055\n",
      "Cumulated Reward: 3.1284527556323747\n",
      "Cumulated Reward: 3.1874728820933464\n",
      "Cumulated Reward: 3.224780728373786\n",
      "Cumulated Reward: 3.289697619057237\n",
      "Cumulated Reward: 3.3492770553933653\n",
      "Cumulated Reward: 3.368400182573109\n",
      "Cumulated Reward: 3.390605086813726\n",
      "Cumulated Reward: 3.415214588464372\n",
      "Cumulated Reward: 3.4536374087657826\n",
      "Cumulated Reward: 3.48402669595648\n",
      "Cumulated Reward: 3.49632205638878\n",
      "Cumulated Reward: 3.5077445325547973\n",
      "Cumulated Reward: 3.535880940661225\n",
      "Cumulated Reward: 3.576119893028533\n",
      "Cumulated Reward: 3.5967931638478614\n",
      "Cumulated Reward: 3.6410113869885397\n",
      "Cumulated Reward: 3.6848309401896477\n",
      "Cumulated Reward: 3.7253907675496123\n",
      "Cumulated Reward: 3.7646124545941144\n",
      "Cumulated Reward: 3.8070296317642796\n",
      "Cumulated Reward: 3.857074885133425\n",
      "Cumulated Reward: 3.89182014222415\n",
      "Cumulated Reward: 3.9323369408447704\n",
      "Cumulated Reward: 4.001001415058246\n",
      "Cumulated Reward: 4.0208523623007615\n",
      "Cumulated Reward: 4.072242478535446\n",
      "Cumulated Reward: 4.1205994555885\n",
      "Cumulated Reward: 4.1944742019985375\n",
      "Cumulated Reward: 4.23268679567767\n",
      "Cumulated Reward: 9.30475271156627\n",
      "Cumulated Reward: 9.373775621689447\n",
      "Cumulated Reward: 9.44049252951596\n",
      "Cumulated Reward: 9.460600985131185\n",
      "Cumulated Reward: 9.498491227337377\n",
      "Cumulated Reward: 9.547466814062261\n",
      "Cumulated Reward: 9.614948902340224\n",
      "Cumulated Reward: 9.644305038093533\n",
      "Cumulated Reward: 9.693020387181347\n",
      "Cumulated Reward: 9.704305808352649\n",
      "Cumulated Reward: 9.628313517312096\n",
      "Cumulated Reward: 9.582944200474955\n",
      "Cumulated Reward: 9.546657370677277\n",
      "Cumulated Reward: 9.48403473383778\n",
      "Cumulated Reward: 9.428207324741821\n",
      "Cumulated Reward: 9.390228806605599\n",
      "Cumulated Reward: 9.33532265785445\n",
      "Cumulated Reward: 9.271193477797688\n",
      "Cumulated Reward: 9.209832111252965\n",
      "Cumulated Reward: 9.162226406577078\n",
      "Cumulated Reward: 9.102966504261433\n",
      "Cumulated Reward: 9.039375129121098\n",
      "Cumulated Reward: 8.997417243861975\n",
      "Cumulated Reward: 8.934813208140259\n",
      "Cumulated Reward: 8.877812784501016\n",
      "Cumulated Reward: 8.808984644274705\n",
      "Cumulated Reward: 8.768856200794424\n",
      "Cumulated Reward: 8.711069797849506\n",
      "Cumulated Reward: 8.665912882203935\n",
      "Cumulated Reward: 8.618956240447543\n",
      "Cumulated Reward: 8.563373850794543\n",
      "Cumulated Reward: 8.49161353861982\n",
      "Cumulated Reward: 8.454984389576403\n",
      "Cumulated Reward: 8.402480472203615\n",
      "Cumulated Reward: 8.378143832252944\n",
      "Cumulated Reward: 8.345869497746387\n",
      "Cumulated Reward: 8.330602827964647\n",
      "Cumulated Reward: 8.274213772069727\n",
      "Cumulated Reward: 8.220888126719853\n",
      "Cumulated Reward: 8.173599652505215\n",
      "Cumulated Reward: 8.128245582293987\n",
      "Cumulated Reward: 8.106757476852946\n",
      "Cumulated Reward: 8.04930016365235\n",
      "Cumulated Reward: 7.996894227197741\n",
      "Cumulated Reward: 7.924141927285516\n",
      "Cumulated Reward: 7.864595235989617\n",
      "Cumulated Reward: 7.843123631749296\n",
      "Cumulated Reward: 7.8039752493554815\n",
      "Cumulated Reward: 7.775020825149192\n",
      "Cumulated Reward: 7.753559662650112\n",
      "Cumulated Reward: 7.719683012645342\n",
      "Cumulated Reward: 7.67880876509458\n",
      "Cumulated Reward: 7.634675351491641\n",
      "Cumulated Reward: 7.608747161003361\n",
      "Cumulated Reward: 7.594295155911942\n",
      "Cumulated Reward: 7.57986303201088\n",
      "Cumulated Reward: 7.562432327836367\n",
      "Cumulated Reward: 7.545950493743835\n",
      "Cumulated Reward: 7.521229355156933\n",
      "Cumulated Reward: 7.511063552309565\n",
      "Cumulated Reward: 7.497957498109454\n",
      "Cumulated Reward: 7.463039804541161\n",
      "Cumulated Reward: 7.440304406286078\n",
      "Cumulated Reward: 7.430194412650502\n",
      "Cumulated Reward: 7.406014878636221\n",
      "Cumulated Reward: 7.396898330317171\n",
      "Cumulated Reward: 7.390402512143043\n",
      "Cumulated Reward: 7.3752141513257286\n",
      "Cumulated Reward: 7.3677670372931825\n",
      "Cumulated Reward: 7.361861950678671\n",
      "Cumulated Reward: 7.3427068064442835\n",
      "Cumulated Reward: 7.336428187839909\n",
      "Cumulated Reward: 7.317039666364893\n",
      "Cumulated Reward: 7.292045290887092\n",
      "Cumulated Reward: 7.262102896584761\n",
      "Cumulated Reward: 7.249504017128337\n",
      "Cumulated Reward: 7.240275917921957\n",
      "Cumulated Reward: 7.238578532642471\n",
      "Cumulated Reward: 7.234707657427633\n",
      "Cumulated Reward: 7.222242598644765\n",
      "Cumulated Reward: 7.199274235431428\n",
      "Cumulated Reward: 7.188863688226275\n",
      "Cumulated Reward: 7.187363081627632\n",
      "Cumulated Reward: 7.189499134915519\n",
      "Cumulated Reward: 7.198752932663721\n",
      "Cumulated Reward: 7.208865783220107\n",
      "Cumulated Reward: 7.22880507040966\n",
      "Cumulated Reward: 7.240051064302419\n",
      "Cumulated Reward: 7.269878800436617\n",
      "Cumulated Reward: 7.29000051406498\n",
      "Cumulated Reward: 7.314608524999007\n",
      "Cumulated Reward: 7.326574645812054\n",
      "Cumulated Reward: 7.336893296096865\n",
      "Cumulated Reward: 7.3453410293820225\n",
      "Cumulated Reward: 7.358881654937772\n",
      "Cumulated Reward: 7.370238952194178\n",
      "Cumulated Reward: 7.384216138499151\n",
      "Cumulated Reward: 7.401129601093\n",
      "Cumulated Reward: 7.421838362173583\n",
      "Cumulated Reward: 7.463335194652334\n",
      "Cumulated Reward: 7.521236109325752\n",
      "Cumulated Reward: 7.555671149394152\n",
      "Cumulated Reward: 7.570137583166395\n",
      "Cumulated Reward: 7.593711506654399\n",
      "Cumulated Reward: 7.624384863249613\n",
      "Cumulated Reward: 7.64654973233511\n",
      "Cumulated Reward: 7.676866565855408\n",
      "Cumulated Reward: 7.713156870824262\n",
      "Cumulated Reward: 7.747137886967449\n",
      "Cumulated Reward: 7.779530742390277\n",
      "Cumulated Reward: 7.811778085841734\n",
      "Cumulated Reward: 7.853577494694625\n",
      "Cumulated Reward: 7.891949050828123\n",
      "Cumulated Reward: 7.927406773742181\n",
      "Cumulated Reward: 7.950548251593721\n",
      "Cumulated Reward: 8.001040858313974\n",
      "Cumulated Reward: 8.04224286371946\n",
      "Cumulated Reward: 8.05904221991557\n",
      "Cumulated Reward: 8.068446282396486\n",
      "Cumulated Reward: 8.100836336867953\n",
      "Cumulated Reward: 8.15848120878612\n",
      "Cumulated Reward: 8.192083156319669\n",
      "Cumulated Reward: 8.209152400399292\n",
      "Cumulated Reward: 8.21307790282986\n",
      "Cumulated Reward: 8.216358956827449\n",
      "Cumulated Reward: 8.230999774688723\n",
      "Cumulated Reward: 8.24322002807909\n",
      "Cumulated Reward: 8.275612500399811\n",
      "Cumulated Reward: 8.30103302834084\n",
      "Cumulated Reward: 8.337613055014314\n",
      "Cumulated Reward: 8.389024190839578\n",
      "Cumulated Reward: 8.407573162897036\n",
      "Cumulated Reward: 8.417056545930011\n",
      "Cumulated Reward: 8.440522028173973\n",
      "Cumulated Reward: 8.450316081771373\n",
      "Cumulated Reward: 8.448281879457701\n",
      "Cumulated Reward: 8.439878871809562\n",
      "Cumulated Reward: 8.421113944243045\n",
      "Cumulated Reward: 8.389807933679586\n",
      "Cumulated Reward: 8.37389295574521\n",
      "Cumulated Reward: 8.363953300728731\n",
      "Cumulated Reward: 8.34327689995632\n",
      "Cumulated Reward: 8.314361121501696\n",
      "Cumulated Reward: 8.305970847588048\n",
      "Cumulated Reward: 8.29862265059658\n",
      "Cumulated Reward: 8.290830180326319\n",
      "Cumulated Reward: 8.273812892732774\n",
      "Cumulated Reward: 8.25381931865944\n",
      "Cumulated Reward: 8.242120232922463\n",
      "Cumulated Reward: 8.201508695274235\n",
      "Cumulated Reward: 8.15059550996121\n",
      "Cumulated Reward: 8.107981405889712\n",
      "Cumulated Reward: 8.051845769826636\n",
      "Cumulated Reward: 7.9989945473815585\n",
      "Cumulated Reward: 7.96741758093852\n",
      "Cumulated Reward: 7.923150782009355\n",
      "Cumulated Reward: 7.877598065337777\n",
      "Cumulated Reward: 7.8472698255492235\n",
      "Cumulated Reward: 7.8087993500327\n",
      "Cumulated Reward: 7.7520305615477385\n",
      "Cumulated Reward: 7.704780560422488\n",
      "Cumulated Reward: 7.690583453565539\n",
      "Cumulated Reward: 7.64419338008995\n",
      "Cumulated Reward: 7.579308299738854\n",
      "Cumulated Reward: 7.53105794529069\n",
      "Cumulated Reward: 7.517520034215255\n",
      "Cumulated Reward: 7.476674361848154\n",
      "Cumulated Reward: 7.449404087405282\n",
      "Cumulated Reward: 7.397325196286992\n",
      "Cumulated Reward: 7.345164308909753\n",
      "Cumulated Reward: 7.311317024764872\n",
      "Cumulated Reward: 7.273526191232049\n",
      "Cumulated Reward: 7.229701285618496\n",
      "Cumulated Reward: 7.193749422380842\n",
      "Cumulated Reward: 7.140210267270831\n",
      "Cumulated Reward: 7.102033659557856\n",
      "Cumulated Reward: 7.0502368695021875\n",
      "Cumulated Reward: 6.977815310040805\n",
      "Cumulated Reward: 6.938601946463217\n",
      "Cumulated Reward: 6.879453163998219\n",
      "Cumulated Reward: 6.823006397706347\n",
      "Cumulated Reward: 6.769855271433498\n",
      "Cumulated Reward: 6.734252883036207\n",
      "Cumulated Reward: 6.707947684439965\n",
      "Cumulated Reward: 6.658107528825133\n",
      "Cumulated Reward: 6.608402577767622\n",
      "Cumulated Reward: 6.600001206805633\n",
      "Cumulated Reward: 6.563118826834282\n",
      "Cumulated Reward: 6.502908923288927\n",
      "Cumulated Reward: 6.45780394259093\n",
      "Cumulated Reward: 6.414949474502563\n",
      "Cumulated Reward: 6.374754679973737\n",
      "Cumulated Reward: 6.325615096859542\n",
      "Cumulated Reward: 6.310629332383552\n",
      "Cumulated Reward: 6.2897562268966904\n",
      "Cumulated Reward: 6.248001104915264\n",
      "Cumulated Reward: 6.210355607329296\n",
      "Cumulated Reward: 6.17513367277191\n",
      "Cumulated Reward: 6.155478060385667\n",
      "Cumulated Reward: 6.148134975998969\n",
      "Cumulated Reward: 6.142527282680206\n",
      "Cumulated Reward: 6.145801652504514\n",
      "Cumulated Reward: 6.153276686487281\n",
      "Cumulated Reward: 6.158709762806299\n",
      "Cumulated Reward: 6.177675918131732\n",
      "Cumulated Reward: 6.203063292741698\n",
      "Cumulated Reward: 6.227486852460768\n",
      "Cumulated Reward: 6.273232875024461\n",
      "Cumulated Reward: 6.317001886705389\n",
      "Cumulated Reward: 6.358137815004838\n",
      "Cumulated Reward: 6.408176764735712\n",
      "Cumulated Reward: 6.450596618092157\n",
      "Cumulated Reward: 6.465437778270682\n",
      "Cumulated Reward: 6.486694526053237\n",
      "Cumulated Reward: 6.523446378569883\n",
      "Cumulated Reward: 6.545001594799686\n",
      "Cumulated Reward: 6.589175237053645\n",
      "Cumulated Reward: 6.623253962714453\n",
      "Cumulated Reward: 6.660132140355589\n",
      "Cumulated Reward: 6.7092321424617065\n",
      "Cumulated Reward: 6.751433304769152\n",
      "Cumulated Reward: 6.784097822305097\n",
      "Cumulated Reward: 6.817921089177288\n",
      "Cumulated Reward: 6.8654773390552\n",
      "Cumulated Reward: 6.905720048163878\n",
      "Cumulated Reward: 6.956292562701272\n",
      "Cumulated Reward: 6.9926798637405065\n",
      "Cumulated Reward: 7.044261135134387\n",
      "Cumulated Reward: 7.068967465540171\n",
      "Cumulated Reward: 7.123911115115885\n",
      "Cumulated Reward: 7.174030297579946\n",
      "Cumulated Reward: 7.220123553243671\n",
      "Cumulated Reward: 7.285907162374093\n",
      "Cumulated Reward: 7.33327338658345\n",
      "Cumulated Reward: 7.373523002378002\n",
      "Cumulated Reward: 7.42036605312925\n",
      "Cumulated Reward: 7.486262309577786\n",
      "Cumulated Reward: 7.51255723280279\n",
      "Cumulated Reward: 7.550184113090461\n",
      "Cumulated Reward: 7.575095294643035\n",
      "Cumulated Reward: 7.617061788659271\n",
      "Cumulated Reward: 7.666116670709014\n",
      "Cumulated Reward: 7.680493072131178\n",
      "Cumulated Reward: 7.719719341532375\n",
      "Cumulated Reward: 7.755707086106925\n",
      "Cumulated Reward: 7.805121233063386\n",
      "Cumulated Reward: 7.852795529719382\n",
      "Cumulated Reward: 7.8995620751857265\n",
      "Cumulated Reward: 7.953995628025185\n",
      "Cumulated Reward: 7.993366030909467\n",
      "Cumulated Reward: 8.015858861249848\n",
      "Cumulated Reward: 8.060367665386527\n",
      "Cumulated Reward: 8.086326023190914\n",
      "Cumulated Reward: 8.111695910557522\n",
      "Cumulated Reward: 8.153622796091224\n",
      "Cumulated Reward: 8.176537775740638\n",
      "Cumulated Reward: 8.216911734488937\n",
      "Cumulated Reward: 8.24021050950738\n",
      "Cumulated Reward: 8.265352489331026\n",
      "Cumulated Reward: 8.27263483272174\n",
      "Cumulated Reward: 8.288899137382248\n",
      "Cumulated Reward: 8.302400430752575\n",
      "Cumulated Reward: 8.319546181559916\n",
      "Cumulated Reward: 8.343132463885594\n",
      "Cumulated Reward: 8.384553270565803\n",
      "Cumulated Reward: 8.403773160246692\n",
      "Cumulated Reward: 8.416218383366699\n",
      "Cumulated Reward: 8.451918016110671\n",
      "Cumulated Reward: 8.488714755564276\n",
      "Cumulated Reward: 8.516444779774979\n",
      "Cumulated Reward: 8.526400670877713\n",
      "Cumulated Reward: 8.556446840014004\n",
      "Cumulated Reward: 8.611557551094641\n",
      "Cumulated Reward: 8.663776166498826\n",
      "Cumulated Reward: 8.693593541989767\n",
      "Cumulated Reward: 8.719669983257965\n",
      "Cumulated Reward: 8.745806890954777\n",
      "Cumulated Reward: 8.774562269952247\n",
      "Cumulated Reward: 8.820407097315464\n",
      "Cumulated Reward: 8.842097788474085\n",
      "Cumulated Reward: 8.87644805908569\n",
      "Cumulated Reward: 8.906631689730197\n",
      "Cumulated Reward: 8.91606747629203\n",
      "Cumulated Reward: 8.942964481430671\n",
      "Cumulated Reward: 8.954130959620915\n",
      "Cumulated Reward: 8.967952573432404\n",
      "Cumulated Reward: 9.003433736710026\n",
      "Cumulated Reward: 9.032214123397518\n",
      "Cumulated Reward: 9.078376746867665\n",
      "Cumulated Reward: 9.111839867302765\n",
      "Cumulated Reward: 9.138712271535763\n",
      "Cumulated Reward: 9.158076034151154\n",
      "Cumulated Reward: 9.17922265936692\n",
      "Cumulated Reward: 9.226126457058513\n",
      "Cumulated Reward: 9.277779382373136\n",
      "Cumulated Reward: 9.329677870850682\n",
      "Cumulated Reward: 9.37272267317706\n",
      "Cumulated Reward: 9.397041196993223\n",
      "Cumulated Reward: 9.451299866848753\n",
      "Cumulated Reward: 9.469666727589681\n",
      "Cumulated Reward: 9.510442619350105\n",
      "Cumulated Reward: 9.52685446930414\n",
      "Cumulated Reward: 9.544629129064365\n",
      "Cumulated Reward: 9.566515744178615\n",
      "Cumulated Reward: 9.588104676046735\n",
      "Cumulated Reward: 9.58922378857564\n",
      "Cumulated Reward: 9.58045788390944\n",
      "Cumulated Reward: 9.560642852978892\n",
      "Cumulated Reward: 9.519173263613467\n",
      "Cumulated Reward: 9.4770874442579\n",
      "Cumulated Reward: 9.426063824115374\n",
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "# --- To evaluate ---\n",
    "custom_goal_pos = [-3.0, -0.0]  # Example custom goal position\n",
    "\n",
    "eval_env = MazeCarEnv(render_mode=\"human\")  # Render during evaluation\n",
    "model = SAC.load(\"models/sac_mazecar_model_2\", env=eval_env)\n",
    "\n",
    "# Set the camera to the same position as for training\n",
    "p.resetDebugVisualizerCamera(cameraDistance=10, cameraYaw=-0.6, cameraPitch=-85, cameraTargetPosition=[0, 0, 0])\n",
    "\n",
    "# Change the starting position\n",
    "eval_env.start_pos = [2.0, -1.0, 0.1]  # New starting position\n",
    "\n",
    "print(\".......................\")\n",
    "obs, info = eval_env.reset(custom_goal_pos=None)\n",
    "print(\".......................\")\n",
    "cumulated_reward = 0.0\n",
    "\n",
    "for _ in range(400):  # Max steps for evaluation\n",
    "    action, _states = model.predict(obs, deterministic=False)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    time.sleep(1./240.0)  # Adjust sleep time for rendering speed\n",
    "    cumulated_reward += reward\n",
    "    print(f\"Cumulated Reward: {cumulated_reward}\")\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Eval episode finished, Reached goal: {info.get('target_goal_index')}, Reward: {reward}\")\n",
    "        obs, info = eval_env.reset(custom_goal_pos=None)  # Reset for next evaluation episode\n",
    "        cumulated_reward = 0.0\n",
    "        print(\".......................\")\n",
    "        print(\".......................\")\n",
    "        print(\".......................\")\n",
    "\n",
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/sac_mazecar_model.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m VecMonitor(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./sac_mazecar_tensorboard/\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Log to the specified directory\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model with the new environment\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSAC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodels/sac_mazecar_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mset_env(env)\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(\n\u001b[1;32m     10\u001b[0m     total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100_000\u001b[39m,\n\u001b[1;32m     11\u001b[0m     reset_num_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     tb_log_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAC_9\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# wichtig: GLEICHER Name!\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/base_class.py:681\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m     get_system_info()\n\u001b[0;32m--> 681\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:403\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_from_zip_file\u001b[39m(\n\u001b[1;32m    377\u001b[0m     load_path: Union[\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath, io\u001b[38;5;241m.\u001b[39mBufferedIOBase],\n\u001b[1;32m    378\u001b[0m     load_data: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     print_system_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]], TensorDict, Optional[TensorDict]]:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    Load model data from a .zip archive\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m        and dict of pytorch variables\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[43mopen_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;66;03m# set device to cpu if cuda is not available\u001b[39;00m\n\u001b[1;32m    406\u001b[0m     device \u001b[38;5;241m=\u001b[39m get_device(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/usr/lib/python3.10/functools.py:889\u001b[0m, in \u001b[0;36msingledispatch.<locals>.wrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires at least \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    887\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1 positional argument\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:240\u001b[0m, in \u001b[0;36mopen_path_str\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@open_path\u001b[39m\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mopen_path_str\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, mode: \u001b[38;5;28mstr\u001b[39m, verbose: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, suffix: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m io\u001b[38;5;241m.\u001b[39mBufferedIOBase:\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    Open a path given by a string. If writing to the path, the function ensures\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    that the path exists.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:291\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    285\u001b[0m         path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# if opening was successful uses the open_path() function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# if opening failed with IsADirectory|FileNotFound, calls open_path_pathlib\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#   with corrections\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# if reading failed with FileNotFoundError, calls open_path_pathlib with suffix\u001b[39;00m\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopen_path_pathlib\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuffix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:272\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    270\u001b[0m             path, suffix \u001b[38;5;241m=\u001b[39m newpath, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    271\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/masterthesis/ve_pybullet/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:264\u001b[0m, in \u001b[0;36mopen_path_pathlib\u001b[0;34m(path, mode, verbose, suffix)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m open_path(\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, mode, verbose, suffix)\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m suffix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m suffix \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1119\u001b[0m, in \u001b[0;36mPath.open\u001b[0;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1118\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[0;32m-> 1119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffering\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/sac_mazecar_model.zip'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    }
   ],
   "source": [
    "# Create a new environment instance\n",
    "env = SubprocVecEnv([make_env(render_mode=None) for _ in range(num_envs)])\n",
    "env = VecMonitor(env, \"./sac_mazecar_tensorboard/\")  # Log to the specified directory\n",
    "\n",
    "# Load the model with the new environment\n",
    "model = SAC.load(\"models/sac_mazecar_model\", env=env)\n",
    "\n",
    "model.set_env(env)\n",
    "model.learn(\n",
    "    total_timesteps=100_000,\n",
    "    reset_num_timesteps=False,\n",
    "    tb_log_name=\"SAC_9\",  # wichtig: GLEICHER Name!\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Continue training the model\n",
    "# model.learn(total_timesteps=100_000, progress_bar=True)\n",
    "\n",
    "# Save the updated model\n",
    "model.save(\"models/sac_mazecar_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mcheck_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# car_pos, _ = p.getBasePositionAndOrientation(eval_env.carId, physicsClientId=eval_env.client)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# p.resetDebugVisualizerCamera(cameraDistance=3, cameraYaw=50, cameraPitch=-35, cameraTargetPosition=car_pos)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "\n",
    "check_env.close()\n",
    "\n",
    "# car_pos, _ = p.getBasePositionAndOrientation(eval_env.carId, physicsClientId=eval_env.client)\n",
    "# p.resetDebugVisualizerCamera(cameraDistance=3, cameraYaw=50, cameraPitch=-35, cameraTargetPosition=car_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numActiveThreads = 0\n",
      "stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n",
      "finished\n",
      "numActiveThreads = 0\n",
      "btShutDownExampleBrowser stopping threads\n",
      "Thread with taskId 0 exiting\n",
      "Thread TERMINATED\n",
      "destroy semaphore\n",
      "semaphore destroyed\n",
      "destroy main semaphore\n",
      "main semaphore destroyed\n"
     ]
    }
   ],
   "source": [
    "eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~/masterthesis/mt_start$   tensorboard --logdir=./ppo_mazecar_tensorboard/\n",
    "# tensorboard --logdir=./sac_mazecar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if GPU is available\n",
    "print(torch.cuda.get_device_name(0))  # Prints the name of the GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_f_mt_start",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
